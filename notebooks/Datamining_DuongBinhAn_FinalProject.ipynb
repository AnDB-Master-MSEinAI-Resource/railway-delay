{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c3f0f8",
   "metadata": {},
   "source": [
    "# Railway Delay Prediction Project\n",
    "\n",
    "This notebook implements a machine learning pipeline for predicting railway delays using various ensemble models.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Configuration](#setup)\n",
    "2. [Data Loading](#data-loading)\n",
    "3. [Exploratory Data Analysis](#eda)\n",
    "4. [Feature Engineering](#feature-engineering)\n",
    "5. [Model Training](#model-training)\n",
    "6. [Model Evaluation](#evaluation)\n",
    "7. [Results and Conclusions](#conclusions)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b66db21",
   "metadata": {},
   "source": [
    "# 1) Introduction — Background, Objectives, and Success Criteria\n",
    "\n",
    "## 1.1 Problem Background\n",
    "\n",
    "Train delays are a critical operational challenge that impacts multiple facets of railway systems:\n",
    "\n",
    "- **Operational Impact**: Delays affect dispatching and scheduling operations, requiring real-time adjustments to maintain service reliability.\n",
    "- **Passenger Experience**: Unpredictable delays diminish customer satisfaction and trust in rail services.\n",
    "- **Resource Optimization**: Inefficient allocation of locomotives and carriages occurs when delays are not accurately anticipated.\n",
    "- **Downstream Connections**: Delay propagation affects connecting services, creating cascading disruptions across the network.\n",
    "\n",
    "Railway operational data exhibits unique characteristics:\n",
    "- **Temporal Nature**: Data is inherently time-dependent, with strong temporal correlations between consecutive observations.\n",
    "- **Chain Effect**: Delays often propagate along routes or across consecutive train services, creating interdependencies that must be captured in predictive models.\n",
    "\n",
    "Understanding and predicting train delays enables proactive decision-making, improved resource allocation, and enhanced passenger communication.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 Objectives\n",
    "\n",
    "This project pursues two complementary analytical approaches to address the railway delay problem:\n",
    "\n",
    "### A. Regression (Primary Task — Recommended)\n",
    "\n",
    "**Objective**: Predict the magnitude of train delays measured in minutes.\n",
    "\n",
    "- **Target Variable**: `DELAY_MINUTES` (or `TARGET` representing delay in minutes)\n",
    "- **Output**: Continuous prediction of delay duration\n",
    "- **Use Case**: Enables precise scheduling adjustments, resource reallocation, and detailed passenger information\n",
    "- **Business Value**: Supports quantitative operational decisions and optimization algorithms\n",
    "\n",
    "### B. Classification (Supporting Task — Optional)\n",
    "\n",
    "**Objective**: Predict whether a train will be delayed based on a business-defined threshold.\n",
    "\n",
    "- **Target Variable**: `IS_DELAYED` (binary: 0 = on-time, 1 = delayed)\n",
    "- **Threshold Definition**: Commonly set at delays exceeding 5 or 10 minutes, depending on operational standards\n",
    "- **Output**: \n",
    "  - Probability of delay occurrence\n",
    "  - Binary delay classification label\n",
    "- **Use Case**: Facilitates alerting systems, early warning mechanisms, and binary decision-making\n",
    "- **Business Value**: Enables proactive interventions and resource mobilization\n",
    "\n",
    "> **Recommendation**: Prioritize the **regression task** as the primary analysis to capture the full spectrum of delay severity. The classification task can complement this by providing binary alerts and supporting threshold-based operational policies.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 Success Criteria\n",
    "\n",
    "Clear evaluation metrics are essential to assess model performance and determine project success:\n",
    "\n",
    "### Regression Task Metrics\n",
    "\n",
    "- **Primary Metrics**:\n",
    "  - **RMSE (Root Mean Squared Error)**: Penalizes large prediction errors, suitable when extreme delays are particularly costly\n",
    "  - **MAE (Mean Absolute Error)**: Provides interpretable average prediction error in minutes\n",
    "  \n",
    "- **Evaluation Protocol**: \n",
    "  - Use **time-based test set** (chronological split) to simulate real-world deployment\n",
    "  - Avoid random shuffling to preserve temporal integrity\n",
    "  - Report metrics separately for different delay ranges (e.g., short delays < 10 min vs. long delays > 30 min)\n",
    "\n",
    "### Classification Task Metrics\n",
    "\n",
    "- **Primary Metrics**:\n",
    "  - **PR-AUC (Precision-Recall Area Under Curve)**: Preferred over ROC-AUC when dealing with imbalanced delay classes\n",
    "  - **F₂ Score**: Emphasizes recall over precision (β=2), appropriate when the business priority is to **detect as many delayed trains as possible**\n",
    "  \n",
    "- **Rationale for F₂ Score**:\n",
    "  - The F_β score is defined as: \n",
    "    $$F_\\beta = (1 + \\beta^2) \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{(\\beta^2 \\cdot \\text{precision}) + \\text{recall}}$$\n",
    "  - With **β = 2**, recall is weighted **twice as much** as precision\n",
    "  - This aligns with operational objectives where **missing a delayed train (false negative) is more costly** than falsely predicting a delay (false positive)\n",
    "  \n",
    "- **Additional Considerations**:\n",
    "  - **Recall (Sensitivity)**: Critical for ensuring comprehensive delay detection\n",
    "  - **Precision**: Balances false alarm rates to maintain operational credibility\n",
    "  - **Threshold Optimization**: Adjust classification threshold to achieve desired precision-recall trade-off\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Success Criteria\n",
    "\n",
    "| Task | Primary Metrics | Evaluation Approach | Target Performance |\n",
    "|------|----------------|---------------------|-------------------|\n",
    "| **Regression** | RMSE, MAE | Time-based test split | Minimize error; benchmark against baseline |\n",
    "| **Classification** | PR-AUC, F₂ Score | Time-based test split | Maximize F₂ (prioritize recall) |\n",
    "\n",
    "**Note**: Success will be measured not only by model performance metrics but also by the model's ability to generalize to future unseen data and provide actionable insights for railway operations.\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Data Description — Overview & Metadata\n",
    "\n",
    "## 2.1 Dataset Overview\n",
    "- Each row represents an event (e.g., one train at one station or one journey segment).\n",
    "- **Temporal columns**: scheduled/actual arrival and departure times.\n",
    "- **Identifiers**: `TRAIN_ID`, `ROUTE_ID`, `STATION_ID`, `TRAIN_NUMBER`.\n",
    "- **Contextual features**: weather conditions, holidays, time slots, operators, vehicle types.\n",
    "- **Target labels**: \n",
    "  - Regression → `DELAY_MINUTES` (or `DELAY_ARRIVAL`)\n",
    "  - Classification → `IS_DELAYED`\n",
    "\n",
    "## 2.2 Data Dictionary (Summary Required)\n",
    "A comprehensive data dictionary should include:\n",
    "- Column name, description, data type, missing value percentage, example values\n",
    "- **Level of measurement**: Nominal / Ordinal / Interval / Ratio — guides encoding and scaling choices\n",
    "\n",
    "## 2.3 Basic Statistics\n",
    "- Mean, median, standard deviation, IQR for numerical features\n",
    "- Check for outliers and right-skewed distributions (consider `log1p` transformation when appropriate)\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Data Preprocessing — Cleaning, Integration, Transformation, Split\n",
    "\n",
    "## 3.1 Data Cleaning\n",
    "- **Missing values**: \n",
    "  - Numeric → median imputation\n",
    "  - Categorical → most frequent value or 'Unknown' category\n",
    "  - Datetime parsing errors → NaT handling\n",
    "- **Noisy data / outliers**: \n",
    "  - Preserve negative values (early arrivals) or clip based on business rules (e.g., clip delay < 0 → 0)\n",
    "  - Apply winsorization for extreme values\n",
    "\n",
    "## 3.2 Integration & Redundancy\n",
    "- When merging multiple sources (schedule, actual, weather), ensure proper entity matching and avoid duplicate keys\n",
    "- Analyze feature correlation to remove redundant variables\n",
    "\n",
    "## 3.3 Transformation\n",
    "- **Scaling**: StandardScaler for linear/SVM/kNN models; tree-based models don't require scaling\n",
    "- **Encoding**: \n",
    "  - OneHotEncoder (with `handle_unknown='ignore'`)\n",
    "  - Target encoding or hashing for high-cardinality features (careful to avoid leakage)\n",
    "- **Target transformation**: Apply `log1p` if target distribution is right-skewed (consider metrics in log-space)\n",
    "\n",
    "## 3.4 Time-Aware Split\n",
    "- **No shuffling**: train = past, test = future\n",
    "- Use `TimeSeriesSplit` for cross-validation to respect temporal ordering\n",
    "\n",
    "---\n",
    "\n",
    "# 4) EDA — Distributions, Temporal Patterns, Correlations\n",
    "\n",
    "- **Univariate**: Histograms and boxplots of target variable (and log-transformed target)\n",
    "- **Bivariate**: Delay patterns by hour, day of week, route, and station\n",
    "- **Temporal**: Rolling mean delay (daily/weekly), trends by month\n",
    "- **Missingness plot** and **correlation heatmap**\n",
    "\n",
    "---\n",
    "\n",
    "# 5) Feature Engineering & Metrics\n",
    "\n",
    "## 5.1 Feature Groups\n",
    "- **Time features**: hour, day_of_week, month + cyclical encoding (sin/cos)\n",
    "- **Lag features**: `PREV_DELAY` per train/route/station\n",
    "- **Rolling features**: 7-day rolling mean/std by route/train; event counts in last 7 days\n",
    "- **Interaction features**: route × peak_hour, station × weather\n",
    "\n",
    "## 5.2 Metrics\n",
    "- **Regression**: RMSE, MAE, R²; MAE@TopDelay (for extreme delays)\n",
    "- **Classification**: PR-AUC, F₂, recall, precision; threshold tuning\n",
    "\n",
    "---\n",
    "\n",
    "# 6) Modeling & Evaluation\n",
    "\n",
    "- **Baselines**: Median predictor, Linear Regression, Ridge\n",
    "- **Tree Ensembles**: RandomForest, XGBoost, LightGBM (CatBoost optional)\n",
    "- **Cross-Validation**: TimeSeriesSplit\n",
    "- **Hyperparameter Tuning**: Optuna (optimize on time-split folds)\n",
    "- **Handling Imbalance** (classification): class_weight, threshold adjustment (cautious with SMOTE to avoid time leakage)\n",
    "\n",
    "---\n",
    "\n",
    "# 7) Comparison, Explainability & Deployment\n",
    "\n",
    "- **Model Comparison**: Evaluate by primary metric, runtime, and stability across folds\n",
    "- **Explainability**: Feature importance analysis + SHAP values (key drivers: peak_hour, route, rolling reliability metrics)\n",
    "- **Deployment**: Save complete `pipeline = preprocessor + model` for inference; implement monitoring for data and performance drift\n",
    "\n",
    "---\n",
    "\n",
    "*Note: Update the data dictionary (metadata table) after loading and inspecting the actual dataset to ensure column names and thresholds align with business requirements.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec098b5",
   "metadata": {},
   "source": [
    "# Cell 1: Import Core Libraries\n",
    "\n",
    "Import essential Python libraries including os for file operations, sys for system-specific parameters, numpy for numerical computations, and pandas for data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41eef062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, numpy as np, pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4be871",
   "metadata": {},
   "source": [
    "# Cell 2: Import Scikit-Learn Modules\n",
    "\n",
    "Import key scikit-learn components: pipeline for building ML pipelines, metrics for evaluation, and preprocessing for data transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae0271ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline, metrics, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4522ba4d",
   "metadata": {},
   "source": [
    "# Cell 3: Import Machine Learning Models\n",
    "\n",
    "Import popular regression models: RandomForestRegressor, XGBRegressor, LGBMRegressor, and CatBoostRegressor for ensemble learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a012e51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: catboost not installed. Install with 'pip install catboost' to enable it.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "# CatBoost may not be installed in all environments — import safely\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    HAS_CATBOOST = True\n",
    "except ModuleNotFoundError:\n",
    "    CatBoostRegressor = None\n",
    "    HAS_CATBOOST = False\n",
    "    print(\"Warning: catboost not installed. Install with 'pip install catboost' to enable it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb01d1",
   "metadata": {},
   "source": [
    "# Cell 4: Import Additional Libraries\n",
    "\n",
    "Import optuna for hyperparameter optimization, joblib for model serialization, matplotlib and seaborn for data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c3362e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna, joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292f1910",
   "metadata": {},
   "source": [
    "# Cell 5: Set Global Random Seed\n",
    "\n",
    "Define a global random state for reproducibility across all random operations in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "975f9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a124cd",
   "metadata": {},
   "source": [
    "# Cell 6: Define Data Paths\n",
    "\n",
    "Set up file paths for the training and testing datasets located in the docs directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19d19449",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r\"D:\\AnDB\\L\\mse\\railway-delay\\docs\"\n",
    "DATATRAIN = os.path.join(DATA_PATH, \"merged_train_data.csv\")\n",
    "DATATEST = os.path.join(DATA_PATH, \"railway-delay-dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad1908f",
   "metadata": {},
   "source": [
    "# Cell 7: Define Column Constants\n",
    "\n",
    "Specify the target column for prediction (delay_minutes) and the date column for time-based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56ec3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = 'DELAY_ARRIVAL'\n",
    "DATE_COL = 'SCHEDULED_DEPARTURE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9d99a",
   "metadata": {},
   "source": [
    "# Cell 8: Memory Optimization Flags\n",
    "\n",
    "Configure flags for handling large datasets: MAX_ROWS for limiting data size and DOWNSAMPLE for reducing sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3cc812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROWS = None\n",
    "DOWNSAMPLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbecffc6",
   "metadata": {},
   "source": [
    "# Cell 9: Create Model Directory\n",
    "\n",
    "Create a directory to store trained models and ensure it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82f055a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13116c1f",
   "metadata": {},
   "source": [
    "# Cell 10: Configuration Sanity Check\n",
    "\n",
    "Verify that all paths are correct, files exist, and print configuration summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b181e3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data paths:\n",
      "Train data: D:\\AnDB\\L\\mse\\railway-delay\\docs\\merged_train_data.csv\n",
      "Test data: D:\\AnDB\\L\\mse\\railway-delay\\docs\\railway-delay-dataset.csv\n",
      "Files exist: Train - True, Test - True\n",
      "Random state: 42\n",
      "Target column: DELAY_ARRIVAL\n",
      "Date column: SCHEDULED_DEPARTURE\n"
     ]
    }
   ],
   "source": [
    "print(\"Data paths:\")\n",
    "print(f\"Train data: {DATATRAIN}\")\n",
    "print(f\"Test data: {DATATEST}\")\n",
    "print(f\"Files exist: Train - {os.path.exists(DATATRAIN)}, Test - {os.path.exists(DATATEST)}\")\n",
    "print(f\"Random state: {RANDOM_STATE}\")\n",
    "print(f\"Target column: {TARGET_COL}\")\n",
    "print(f\"Date column: {DATE_COL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff013d50",
   "metadata": {},
   "source": [
    "# Group B — Load Data (Cells 11–20)\n",
    "\n",
    "This section loads training and test datasets, detects and parses datetime columns, extracts time-based features (hour, weekday), and performs basic checks and sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2379aebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 11: Read train CSV ---\n",
      "Loaded train: (9312671, 31)\n",
      "Cell 11 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 – Read CSV (train)\n",
    "print('\\n--- Cell 11: Read train CSV ---')\n",
    "try:\n",
    "    df_train = pd.read_csv(DATATRAIN, nrows=MAX_ROWS, low_memory=False)\n",
    "    print('Loaded train:', df_train.shape)\n",
    "except Exception as e:\n",
    "    print('Error reading train file:', e)\n",
    "    df_train = None\n",
    "\n",
    "if df_train is None:\n",
    "    raise FileNotFoundError(f\"Train file not found or could not be read: {DATATRAIN}\")\n",
    "\n",
    "if DOWNSAMPLE:\n",
    "    print('Downsampling: using 10% sample of train')\n",
    "    df_train = df_train.sample(frac=0.1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "TRAIN_DF = df_train\n",
    "print('Cell 11 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8243a4",
   "metadata": {},
   "source": [
    "# Cell 12: Read Test CSV (if available)\n",
    "\n",
    "Read the test dataset if it exists; otherwise continue with train-only workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1921d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 12: Read test CSV (if exists) ---\n",
      "Loaded test: (5819078, 31)\n",
      "Cell 12 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12 – Read CSV (test, nếu có)\n",
    "print('\\n--- Cell 12: Read test CSV (if exists) ---')\n",
    "if os.path.exists(DATATEST):\n",
    "    try:\n",
    "        df_test = pd.read_csv(DATATEST, nrows=MAX_ROWS, low_memory=False)\n",
    "        print('Loaded test:', df_test.shape)\n",
    "    except Exception as e:\n",
    "        print('Error reading test file:', e)\n",
    "        df_test = None\n",
    "else:\n",
    "    df_test = None\n",
    "    print('Test file not found; proceeding without test set')\n",
    "\n",
    "TEST_DF = df_test\n",
    "print('Cell 12 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc975a68",
   "metadata": {},
   "source": [
    "# Cell 13: Print columns & shapes\n",
    "\n",
    "Quick check of columns and shapes for train and test dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba3fe85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 13: Train columns & shape ---\n",
      "Train shape: (9312671, 31)\n",
      "Train columns: ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'TRAIN_OPERATOR', 'TRAIN_NUMBER', 'COACH_ID', 'SOURCE_STATION', 'DESTINATION_STATION', 'SCHEDULED_DEPARTURE', 'ACTUAL_DEPARTURE', 'DELAY_DEPARTURE', 'PLATFORM_TIME_OUT', 'TRAIN_DEPARTURE_EVENT', 'SCHEDULED_TIME', 'ELAPSED_TIME', 'RUN_TIME', 'DISTANCE_KM', 'LEFT_SOURCE_STATION_TIME', 'PLATFORM_TIME_IN', 'SCHEDULED_ARRIVAL', 'ACTUAL_ARRIVAL', 'DELAY_ARRIVAL', 'DIVERTED', 'CANCELLED', 'CANCELLATION_REASON', 'SYSTEM_DELAY', 'SECURITY_DELAY', 'TRAIN_OPERATOR_DELAY', 'LATE_TRAIN_DELAY', 'WEATHER_DELAY']\n",
      "\n",
      "--- Test columns & shape ---\n",
      "Test shape: (5819078, 31)\n",
      "Test columns: ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'TRAIN_OPERATOR', 'TRAIN_NUMBER', 'COACH_ID', 'SOURCE_STATION', 'DESTINATION_STATION', 'SCHEDULED_DEPARTURE', 'ACTUAL_DEPARTURE', 'DELAY_DEPARTURE', 'PLATFORM_TIME_OUT', 'TRAIN_DEPARTURE_EVENT', 'SCHEDULED_TIME', 'ELAPSED_TIME', 'RUN_TIME', 'DISTANCE_KM', 'LEFT_SOURCE_STATION_TIME', 'PLATFORM_TIME_IN', 'SCHEDULED_ARRIVAL', 'ACTUAL_ARRIVAL', 'DELAY_ARRIVAL', 'DIVERTED', 'CANCELLED', 'CANCELLATION_REASON', 'SYSTEM_DELAY', 'SECURITY_DELAY', 'TRAIN_OPERATOR_DELAY', 'LATE_TRAIN_DELAY', 'WEATHER_DELAY']\n",
      "Cell 13 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13 – Print columns\n",
    "print('\\n--- Cell 13: Train columns & shape ---')\n",
    "print('Train shape:', TRAIN_DF.shape)\n",
    "print('Train columns:', TRAIN_DF.columns.tolist())\n",
    "if TEST_DF is not None:\n",
    "    print('\\n--- Test columns & shape ---')\n",
    "    print('Test shape:', TEST_DF.shape)\n",
    "    print('Test columns:', TEST_DF.columns.tolist())\n",
    "print('Cell 13 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd85a5c",
   "metadata": {},
   "source": [
    "# Cell 14: Auto-detect datetime column\n",
    "\n",
    "Define a small helper to auto-detect datetime-like columns and choose the preferred one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eddc3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected datetime columns -> train: SCHEDULED_DEPARTURE , test: SCHEDULED_DEPARTURE\n",
      "Cell 14 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14 – Auto-detect datetime column\n",
    "import re\n",
    "\n",
    "def detect_datetime_col(df, preferred=None):\n",
    "    if df is None:\n",
    "        return None\n",
    "    if preferred and preferred in df.columns:\n",
    "        return preferred\n",
    "    candidates = [c for c in df.columns if re.search(r'date|time|scheduled|departure|arrival', c, flags=re.I)]\n",
    "    if not candidates:\n",
    "        return None\n",
    "    for p in ['scheduled_departure','scheduled_time','scheduled_arrival','scheduled']:\n",
    "        for c in candidates:\n",
    "            if p in c.lower():\n",
    "                return c\n",
    "    return candidates[0]\n",
    "\n",
    "TRAIN_DT_COL = detect_datetime_col(TRAIN_DF, preferred=DATE_COL)\n",
    "TEST_DT_COL = detect_datetime_col(TEST_DF, preferred=DATE_COL) if TEST_DF is not None else None\n",
    "print('Detected datetime columns -> train:', TRAIN_DT_COL, ', test:', TEST_DT_COL)\n",
    "print('Cell 14 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1f20c6",
   "metadata": {},
   "source": [
    "# Cell 15: Parse datetime with pd.to_datetime\n",
    "\n",
    "Coerce the detected datetime column(s) using pandas' to_datetime with error coercion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37bd2f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20728\\1253054865.py:3: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  TRAIN_DF[TRAIN_DT_COL] = pd.to_datetime(TRAIN_DF[TRAIN_DT_COL].astype(str), errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20728\\1253054865.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  TRAIN_DF[TRAIN_DT_COL] = pd.to_datetime(TRAIN_DF[TRAIN_DT_COL].astype(str), errors='coerce', infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed train datetime:  2592697 non-missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20728\\1253054865.py:9: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  TEST_DF[TEST_DT_COL] = pd.to_datetime(TEST_DF[TEST_DT_COL].astype(str), errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20728\\1253054865.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  TEST_DF[TEST_DT_COL] = pd.to_datetime(TEST_DF[TEST_DT_COL].astype(str), errors='coerce', infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed test datetime:  1620530 non-missing\n",
      "Cell 15 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 15 – pd.to_datetime\n",
    "if TRAIN_DT_COL is not None:\n",
    "    TRAIN_DF[TRAIN_DT_COL] = pd.to_datetime(TRAIN_DF[TRAIN_DT_COL].astype(str), errors='coerce', infer_datetime_format=True)\n",
    "    print('Parsed train datetime: ', TRAIN_DF[TRAIN_DT_COL].notna().sum(), 'non-missing')\n",
    "else:\n",
    "    print('No datetime detected for train; skipping parse')\n",
    "\n",
    "if TEST_DF is not None and TEST_DT_COL is not None:\n",
    "    TEST_DF[TEST_DT_COL] = pd.to_datetime(TEST_DF[TEST_DT_COL].astype(str), errors='coerce', infer_datetime_format=True)\n",
    "    print('Parsed test datetime: ', TEST_DF[TEST_DT_COL].notna().sum(), 'non-missing')\n",
    "\n",
    "print('Cell 15 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7e08fb",
   "metadata": {},
   "source": [
    "# Cell 16: Extract hour and weekday\n",
    "\n",
    "Create HOUR and WEEKDAY features from parsed datetime column(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2fa8963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added train hour/weekday columns\n",
      "Added test hour/weekday columns\n",
      "Cell 16 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 16 – Extract hour / weekday\n",
    "if TRAIN_DT_COL is not None:\n",
    "    TRAIN_DF[f'{TRAIN_DT_COL}_HOUR'] = TRAIN_DF[TRAIN_DT_COL].dt.hour\n",
    "    TRAIN_DF[f'{TRAIN_DT_COL}_WEEKDAY'] = TRAIN_DF[TRAIN_DT_COL].dt.dayofweek\n",
    "    print('Added train hour/weekday columns')\n",
    "else:\n",
    "    print('Train datetime not available; skipping hour/weekday extraction')\n",
    "\n",
    "if TEST_DF is not None and TEST_DT_COL is not None:\n",
    "    TEST_DF[f'{TEST_DT_COL}_HOUR'] = TEST_DF[TEST_DT_COL].dt.hour\n",
    "    TEST_DF[f'{TEST_DT_COL}_WEEKDAY'] = TEST_DF[TEST_DT_COL].dt.dayofweek\n",
    "    print('Added test hour/weekday columns')\n",
    "\n",
    "print('Cell 16 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c182263",
   "metadata": {},
   "source": [
    "# Cell 17: Handle missing datetime\n",
    "\n",
    "Attempt to fill missing datetime values from YEAR/MONTH/DAY + scheduled time columns (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87c1c199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datetime missing before: 6719974, after: 6719974\n",
      "Test datetime missing before: 4198548, after: 4198548\n",
      "Cell 17 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 17 – Handle missing datetime (fill from YEAR/MONTH/DAY + time)\n",
    "\n",
    "def try_fill_datetime_from_parts(df, dt_col, time_cols_order=None):\n",
    "    if df is None or dt_col is None:\n",
    "        return df\n",
    "    if not {'YEAR','MONTH','DAY'}.issubset(set(df.columns)):\n",
    "        return df\n",
    "    if time_cols_order is None:\n",
    "        time_cols_order = ['SCHEDULED_DEPARTURE','SCHEDULED_TIME','SCHEDULED_ARRIVAL','ACTUAL_DEPARTURE','ACTUAL_ARRIVAL']\n",
    "    time_cols = [c for c in time_cols_order if c in df.columns]\n",
    "    if not time_cols:\n",
    "        return df\n",
    "    time_col = time_cols[0]\n",
    "\n",
    "    def build_dt(row):\n",
    "        try:\n",
    "            y = int(row['YEAR'])\n",
    "            m = int(row['MONTH'])\n",
    "            d = int(row['DAY'])\n",
    "            t = row[time_col]\n",
    "            if pd.isna(t):\n",
    "                return pd.NaT\n",
    "            t_str = str(int(float(t)))\n",
    "            if len(t_str) <= 2:\n",
    "                hour = int(t_str)\n",
    "                minute = 0\n",
    "            else:\n",
    "                if len(t_str) in (3,4):\n",
    "                    hour = int(t_str[:-2])\n",
    "                    minute = int(t_str[-2:])\n",
    "                else:\n",
    "                    hour = int(t_str)\n",
    "                    minute = 0\n",
    "            return pd.Timestamp(year=y, month=m, day=d, hour=hour, minute=minute)\n",
    "        except Exception:\n",
    "            return pd.NaT\n",
    "\n",
    "    mask_missing = df[dt_col].isna()\n",
    "    if mask_missing.any():\n",
    "        df.loc[mask_missing, dt_col] = df[mask_missing].apply(build_dt, axis=1)\n",
    "    return df\n",
    "\n",
    "# Apply to train/test\n",
    "if TRAIN_DT_COL is not None:\n",
    "    miss_before = TRAIN_DF[TRAIN_DT_COL].isna().sum()\n",
    "    TRAIN_DF = try_fill_datetime_from_parts(TRAIN_DF, TRAIN_DT_COL)\n",
    "    miss_after = TRAIN_DF[TRAIN_DT_COL].isna().sum()\n",
    "    print(f'Train datetime missing before: {miss_before}, after: {miss_after}')\n",
    "\n",
    "if TEST_DF is not None and TEST_DT_COL is not None:\n",
    "    miss_before = TEST_DF[TEST_DT_COL].isna().sum()\n",
    "    TEST_DF = try_fill_datetime_from_parts(TEST_DF, TEST_DT_COL)\n",
    "    miss_after = TEST_DF[TEST_DT_COL].isna().sum()\n",
    "    print(f'Test datetime missing before: {miss_before}, after: {miss_after}')\n",
    "\n",
    "print('Cell 17 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c128db19",
   "metadata": {},
   "source": [
    "# Cell 18: Sort by time\n",
    "\n",
    "Sort the dataframe by the parsed datetime column and reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cacaa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sorted by SCHEDULED_DEPARTURE\n",
      "Test sorted by SCHEDULED_DEPARTURE\n",
      "Cell 18 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 18 – Sort by time\n",
    "if TRAIN_DT_COL is not None:\n",
    "    TRAIN_DF = TRAIN_DF.sort_values(by=TRAIN_DT_COL).reset_index(drop=True)\n",
    "    print('Train sorted by', TRAIN_DT_COL)\n",
    "else:\n",
    "    print('Train datetime missing; skipping sort')\n",
    "\n",
    "if TEST_DF is not None and TEST_DT_COL is not None:\n",
    "    TEST_DF = TEST_DF.sort_values(by=TEST_DT_COL).reset_index(drop=True)\n",
    "    print('Test sorted by', TEST_DT_COL)\n",
    "\n",
    "print('Cell 18 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9eeb0a",
   "metadata": {},
   "source": [
    "# Cell 19: Basic data info\n",
    "\n",
    "Show dataframe info and top missing counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e36fc13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Train info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9312671 entries, 0 to 9312670\n",
      "Data columns (total 33 columns):\n",
      " #   Column                       Dtype         \n",
      "---  ------                       -----         \n",
      " 0   YEAR                         object        \n",
      " 1   MONTH                        object        \n",
      " 2   DAY                          object        \n",
      " 3   DAY_OF_WEEK                  object        \n",
      " 4   TRAIN_OPERATOR               object        \n",
      " 5   TRAIN_NUMBER                 object        \n",
      " 6   COACH_ID                     object        \n",
      " 7   SOURCE_STATION               object        \n",
      " 8   DESTINATION_STATION          object        \n",
      " 9   SCHEDULED_DEPARTURE          datetime64[ns]\n",
      " 10  ACTUAL_DEPARTURE             object        \n",
      " 11  DELAY_DEPARTURE              object        \n",
      " 12  PLATFORM_TIME_OUT            object        \n",
      " 13  TRAIN_DEPARTURE_EVENT        object        \n",
      " 14  SCHEDULED_TIME               object        \n",
      " 15  ELAPSED_TIME                 object        \n",
      " 16  RUN_TIME                     object        \n",
      " 17  DISTANCE_KM                  object        \n",
      " 18  LEFT_SOURCE_STATION_TIME     object        \n",
      " 19  PLATFORM_TIME_IN             object        \n",
      " 20  SCHEDULED_ARRIVAL            object        \n",
      " 21  ACTUAL_ARRIVAL               object        \n",
      " 22  DELAY_ARRIVAL                object        \n",
      " 23  DIVERTED                     object        \n",
      " 24  CANCELLED                    object        \n",
      " 25  CANCELLATION_REASON          object        \n",
      " 26  SYSTEM_DELAY                 object        \n",
      " 27  SECURITY_DELAY               object        \n",
      " 28  TRAIN_OPERATOR_DELAY         object        \n",
      " 29  LATE_TRAIN_DELAY             object        \n",
      " 30  WEATHER_DELAY                object        \n",
      " 31  SCHEDULED_DEPARTURE_HOUR     float64       \n",
      " 32  SCHEDULED_DEPARTURE_WEEKDAY  float64       \n",
      "dtypes: datetime64[ns](1), float64(2), object(30)\n",
      "memory usage: 2.3+ GB\n",
      "\n",
      "Top missing counts (train):\n",
      "CANCELLATION_REASON            9168848\n",
      "TRAIN_OPERATOR_DELAY           7610182\n",
      "SECURITY_DELAY                 7610182\n",
      "LATE_TRAIN_DELAY               7610182\n",
      "WEATHER_DELAY                  7610182\n",
      "SYSTEM_DELAY                   7610182\n",
      "SCHEDULED_DEPARTURE_HOUR       6719974\n",
      "SCHEDULED_DEPARTURE            6719974\n",
      "SCHEDULED_DEPARTURE_WEEKDAY    6719974\n",
      "ELAPSED_TIME                    168148\n",
      "RUN_TIME                        168142\n",
      "DELAY_ARRIVAL                   168116\n",
      "LEFT_SOURCE_STATION_TIME        147990\n",
      "PLATFORM_TIME_IN                147990\n",
      "ACTUAL_ARRIVAL                  147985\n",
      "dtype: int64\n",
      "\n",
      "--- Test info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5819078 entries, 0 to 5819077\n",
      "Data columns (total 33 columns):\n",
      " #   Column                       Dtype         \n",
      "---  ------                       -----         \n",
      " 0   YEAR                         int64         \n",
      " 1   MONTH                        int64         \n",
      " 2   DAY                          int64         \n",
      " 3   DAY_OF_WEEK                  int64         \n",
      " 4   TRAIN_OPERATOR               object        \n",
      " 5   TRAIN_NUMBER                 int64         \n",
      " 6   COACH_ID                     object        \n",
      " 7   SOURCE_STATION               object        \n",
      " 8   DESTINATION_STATION          object        \n",
      " 9   SCHEDULED_DEPARTURE          datetime64[ns]\n",
      " 10  ACTUAL_DEPARTURE             float64       \n",
      " 11  DELAY_DEPARTURE              float64       \n",
      " 12  PLATFORM_TIME_OUT            float64       \n",
      " 13  TRAIN_DEPARTURE_EVENT        float64       \n",
      " 14  SCHEDULED_TIME               float64       \n",
      " 15  ELAPSED_TIME                 float64       \n",
      " 16  RUN_TIME                     float64       \n",
      " 17  DISTANCE_KM                  int64         \n",
      " 18  LEFT_SOURCE_STATION_TIME     float64       \n",
      " 19  PLATFORM_TIME_IN             float64       \n",
      " 20  SCHEDULED_ARRIVAL            int64         \n",
      " 21  ACTUAL_ARRIVAL               float64       \n",
      " 22  DELAY_ARRIVAL                float64       \n",
      " 23  DIVERTED                     int64         \n",
      " 24  CANCELLED                    int64         \n",
      " 25  CANCELLATION_REASON          object        \n",
      " 26  SYSTEM_DELAY                 float64       \n",
      " 27  SECURITY_DELAY               float64       \n",
      " 28  TRAIN_OPERATOR_DELAY         float64       \n",
      " 29  LATE_TRAIN_DELAY             float64       \n",
      " 30  WEATHER_DELAY                float64       \n",
      " 31  SCHEDULED_DEPARTURE_HOUR     float64       \n",
      " 32  SCHEDULED_DEPARTURE_WEEKDAY  float64       \n",
      "dtypes: datetime64[ns](1), float64(18), int64(9), object(5)\n",
      "memory usage: 1.4+ GB\n",
      "\n",
      "Top missing counts (test):\n",
      "CANCELLATION_REASON            5729194\n",
      "TRAIN_OPERATOR_DELAY           4755639\n",
      "SECURITY_DELAY                 4755639\n",
      "LATE_TRAIN_DELAY               4755639\n",
      "WEATHER_DELAY                  4755639\n",
      "SYSTEM_DELAY                   4755639\n",
      "SCHEDULED_DEPARTURE_HOUR       4198548\n",
      "SCHEDULED_DEPARTURE            4198548\n",
      "SCHEDULED_DEPARTURE_WEEKDAY    4198548\n",
      "ELAPSED_TIME                    105071\n",
      "RUN_TIME                        105071\n",
      "DELAY_ARRIVAL                   105071\n",
      "ACTUAL_ARRIVAL                   92513\n",
      "PLATFORM_TIME_IN                 92513\n",
      "LEFT_SOURCE_STATION_TIME         92513\n",
      "dtype: int64\n",
      "Cell 19 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 19 – Basic data info (df.info())\n",
    "print('\\n--- Train info ---')\n",
    "TRAIN_DF.info()\n",
    "print('\\nTop missing counts (train):')\n",
    "print(TRAIN_DF.isna().sum().sort_values(ascending=False).head(15))\n",
    "\n",
    "if TEST_DF is not None:\n",
    "    print('\\n--- Test info ---')\n",
    "    TEST_DF.info()\n",
    "    print('\\nTop missing counts (test):')\n",
    "    print(TEST_DF.isna().sum().sort_values(ascending=False).head(15))\n",
    "\n",
    "print('Cell 19 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821b85d3",
   "metadata": {},
   "source": [
    "# Cell 20: Head / sample view\n",
    "\n",
    "Display the first rows and a small random sample to inspect data content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc06a870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Train head ---\n",
      "   YEAR MONTH DAY DAY_OF_WEEK TRAIN_OPERATOR TRAIN_NUMBER COACH_ID  \\\n",
      "0  2015     1   1           4             UA          144   N66056   \n",
      "1  2015     4  13           1             AA          247   N5EDAA   \n",
      "2  2015     4  13           1             AA          243   N476AA   \n",
      "3  2015     4  13           1             AA          199   N3DHAA   \n",
      "4  2015     4  13           1             AA           45   N3CPAA   \n",
      "\n",
      "  SOURCE_STATION DESTINATION_STATION SCHEDULED_DEPARTURE  ... DIVERTED  \\\n",
      "0            HNL                 IAD          1700-01-01  ...        0   \n",
      "1            LAX                 KOA          1700-01-01  ...        0   \n",
      "2            DFW                 SAT          1700-01-01  ...        0   \n",
      "3            JFK                 ORD          1700-01-01  ...        0   \n",
      "4            JFK                 LAS          1700-01-01  ...        0   \n",
      "\n",
      "  CANCELLED CANCELLATION_REASON SYSTEM_DELAY SECURITY_DELAY  \\\n",
      "0         0                 NaN          NaN            NaN   \n",
      "1         0                 NaN          NaN            NaN   \n",
      "2         0                 NaN          NaN            NaN   \n",
      "3         0                 NaN          NaN            NaN   \n",
      "4         0                 NaN          0.0            0.0   \n",
      "\n",
      "  TRAIN_OPERATOR_DELAY LATE_TRAIN_DELAY WEATHER_DELAY  \\\n",
      "0                  NaN              NaN           NaN   \n",
      "1                  NaN              NaN           NaN   \n",
      "2                  NaN              NaN           NaN   \n",
      "3                  NaN              NaN           NaN   \n",
      "4                 17.0             51.0           0.0   \n",
      "\n",
      "  SCHEDULED_DEPARTURE_HOUR SCHEDULED_DEPARTURE_WEEKDAY  \n",
      "0                      0.0                         4.0  \n",
      "1                      0.0                         4.0  \n",
      "2                      0.0                         4.0  \n",
      "3                      0.0                         4.0  \n",
      "4                      0.0                         4.0  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "\n",
      "--- Train random sample ---\n",
      "         YEAR MONTH DAY DAY_OF_WEEK TRAIN_OPERATOR TRAIN_NUMBER COACH_ID  \\\n",
      "4833228  2015     8  30           7             AA         2062   N749US   \n",
      "9203211  2015     5  12           2             OO         5301   N943SW   \n",
      "6913993  2015    12  19           6             WN           30   N291WN   \n",
      "382433   2015     7   6           1             EV         6022   N27200   \n",
      "4488638  2015     7  24           5             WN          850   N916WN   \n",
      "\n",
      "        SOURCE_STATION DESTINATION_STATION SCHEDULED_DEPARTURE  ... DIVERTED  \\\n",
      "4833228            EWR                 CLT                 NaT  ...        0   \n",
      "9203211            MKE                 DEN                 NaT  ...        0   \n",
      "6913993            OAK                 LAX                 NaT  ...        0   \n",
      "382433             RIC                 ORD          1735-01-01  ...        0   \n",
      "4488638            MSP                 DEN                 NaT  ...        0   \n",
      "\n",
      "        CANCELLED CANCELLATION_REASON SYSTEM_DELAY SECURITY_DELAY  \\\n",
      "4833228         0                 NaN          NaN            NaN   \n",
      "9203211         0                 NaN          NaN            NaN   \n",
      "6913993         0                 NaN          0.0            0.0   \n",
      "382433          0                 NaN          0.0            0.0   \n",
      "4488638         0                 NaN          0.0            0.0   \n",
      "\n",
      "        TRAIN_OPERATOR_DELAY LATE_TRAIN_DELAY WEATHER_DELAY  \\\n",
      "4833228                  NaN              NaN           NaN   \n",
      "9203211                  NaN              NaN           NaN   \n",
      "6913993                 10.0             63.0           0.0   \n",
      "382433                   0.0             85.0           0.0   \n",
      "4488638                 10.0             18.0           0.0   \n",
      "\n",
      "        SCHEDULED_DEPARTURE_HOUR SCHEDULED_DEPARTURE_WEEKDAY  \n",
      "4833228                      NaN                         NaN  \n",
      "9203211                      NaN                         NaN  \n",
      "6913993                      NaN                         NaN  \n",
      "382433                       0.0                         5.0  \n",
      "4488638                      NaN                         NaN  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "\n",
      "--- Test head ---\n",
      "   YEAR  MONTH  DAY  DAY_OF_WEEK TRAIN_OPERATOR  TRAIN_NUMBER COACH_ID  \\\n",
      "0  2015      1    1            4             UA           144   N66056   \n",
      "1  2015      4   23            4             US          2149   N825AW   \n",
      "2  2015      4   23            4             US          2153   N956UW   \n",
      "3  2015      4   23            4             US          2164   N950UW   \n",
      "4  2015      4   23            4             US          2172   N949UW   \n",
      "\n",
      "  SOURCE_STATION DESTINATION_STATION SCHEDULED_DEPARTURE  ...  DIVERTED  \\\n",
      "0            HNL                 IAD          1700-01-01  ...         0   \n",
      "1            BOS                 DCA          1700-01-01  ...         0   \n",
      "2            DCA                 LGA          1700-01-01  ...         0   \n",
      "3            LGA                 DCA          1700-01-01  ...         0   \n",
      "4            BOS                 LGA          1700-01-01  ...         0   \n",
      "\n",
      "   CANCELLED  CANCELLATION_REASON  SYSTEM_DELAY  SECURITY_DELAY  \\\n",
      "0          0                  NaN           NaN             NaN   \n",
      "1          0                  NaN           NaN             NaN   \n",
      "2          0                  NaN           NaN             NaN   \n",
      "3          0                  NaN           NaN             NaN   \n",
      "4          0                  NaN           NaN             NaN   \n",
      "\n",
      "   TRAIN_OPERATOR_DELAY  LATE_TRAIN_DELAY  WEATHER_DELAY  \\\n",
      "0                   NaN               NaN            NaN   \n",
      "1                   NaN               NaN            NaN   \n",
      "2                   NaN               NaN            NaN   \n",
      "3                   NaN               NaN            NaN   \n",
      "4                   NaN               NaN            NaN   \n",
      "\n",
      "   SCHEDULED_DEPARTURE_HOUR  SCHEDULED_DEPARTURE_WEEKDAY  \n",
      "0                       0.0                          4.0  \n",
      "1                       0.0                          4.0  \n",
      "2                       0.0                          4.0  \n",
      "3                       0.0                          4.0  \n",
      "4                       0.0                          4.0  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "Cell 20 completed. Group B finished.\n"
     ]
    }
   ],
   "source": [
    "# Cell 20 – Head / sample view\n",
    "print('\\n--- Train head ---')\n",
    "print(TRAIN_DF.head())\n",
    "print('\\n--- Train random sample ---')\n",
    "print(TRAIN_DF.sample(min(5, len(TRAIN_DF)), random_state=RANDOM_STATE))\n",
    "\n",
    "if TEST_DF is not None:\n",
    "    print('\\n--- Test head ---')\n",
    "    print(TEST_DF.head())\n",
    "\n",
    "print('Cell 20 completed. Group B finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97727955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group C helpers (Cells 21–29) added.\n"
     ]
    }
   ],
   "source": [
    "# Cells 21–29: Helper functions for feature engineering, metrics, plotting, and inference\n",
    "import typing as t\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, auc, confusion_matrix, fbeta_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# Cell 21 – _get_route_column\n",
    "def _get_route_column(df: pd.DataFrame, src_col: str = 'SOURCE_STATION', dst_col: str = 'DESTINATION_STATION', train_col: str = 'TRAIN_NUMBER', route_col: str = 'ROUTE') -> str:\n",
    "    \"\"\"Create or return a route identifier column name.\n",
    "\n",
    "    Preference: if both source and destination exist, build 'SRC_DST'. Else fall back to TRAIN_NUMBER.\n",
    "    Returns the route column name (added if not present).\n",
    "    \"\"\"\n",
    "    if src_col in df.columns and dst_col in df.columns:\n",
    "        if route_col not in df.columns:\n",
    "            df[route_col] = df[src_col].astype(str) + '_' + df[dst_col].astype(str)\n",
    "        return route_col\n",
    "    if train_col in df.columns:\n",
    "        return train_col\n",
    "    raise ValueError('Neither route nor train identifier columns found')\n",
    "\n",
    "\n",
    "# Cell 22 – compute_prev_delay_safe\n",
    "def compute_prev_delay_safe(df: pd.DataFrame, groupby_cols: t.List[str], dt_col: str, delay_col: str, out_col: str = 'PREV_DELAY') -> pd.DataFrame:\n",
    "    \"\"\"Compute previous delay (lag-1) per group in a time-safe manner.\n",
    "\n",
    "    Assumes dt_col is parsed datetime and df is sorted by dt_col.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if dt_col not in df.columns or delay_col not in df.columns:\n",
    "        df[out_col] = np.nan\n",
    "        return df\n",
    "    df = df.sort_values(by=dt_col)\n",
    "    df[out_col] = df.groupby(groupby_cols)[delay_col].shift(1)\n",
    "    # option: fill missing with 0 or keep NaN — keep NaN to avoid leakage\n",
    "    return df\n",
    "\n",
    "\n",
    "# Cell 23 – compute_rolling_features_safe\n",
    "def compute_rolling_features_safe(df: pd.DataFrame, groupby_col: str, dt_col: str, value_col: str, window: str = '7D', min_periods: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"Compute rolling mean/std/count over a time window per group (time-based rolling).\n",
    "\n",
    "    Returns new columns: ROLL_MEAN_{window}, ROLL_STD_{window}, ROLL_COUNT_{window}\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if dt_col not in df.columns or value_col not in df.columns:\n",
    "        return df\n",
    "    # ensure datetime index\n",
    "    df = df.sort_values(by=dt_col).reset_index(drop=True)\n",
    "    col_mean = f'ROLL_MEAN_{window}'\n",
    "    col_std = f'ROLL_STD_{window}'\n",
    "    col_count = f'ROLL_COUNT_{window}'\n",
    "\n",
    "    # set index for rolling\n",
    "    tmp = df.set_index(pd.DatetimeIndex(df[dt_col]))\n",
    "    rolled = tmp.groupby(groupby_col)[value_col].rolling(window=window, min_periods=min_periods).agg(['mean','std','count']).reset_index(level=0, drop=True)\n",
    "    df[col_mean] = rolled['mean'].values\n",
    "    df[col_std] = rolled['std'].values\n",
    "    df[col_count] = rolled['count'].values\n",
    "    return df\n",
    "\n",
    "\n",
    "# Cell 24 – metrics_summary\n",
    "def metrics_summary_regression(y_true: t.Iterable, y_pred: t.Iterable) -> dict:\n",
    "    return {\n",
    "        'rmse': mean_squared_error(y_true, y_pred, squared=False),\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'r2': r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "def metrics_summary_classification(y_true: t.Iterable, y_proba: t.Iterable, threshold: float = 0.5, beta: float = 2.0) -> dict:\n",
    "    y_pred = (np.array(y_proba) >= threshold).astype(int)\n",
    "    ap = average_precision_score(y_true, y_proba)\n",
    "    fbeta = fbeta_score(y_true, y_pred, beta=beta)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    return {'ap': ap, 'fbeta': fbeta, 'precision': prec, 'recall': rec, 'threshold': threshold}\n",
    "\n",
    "\n",
    "# Cell 25 – plotting helpers (PR / ROC)\n",
    "def plot_pr_roc(y_true: t.Iterable, y_score: t.Iterable, figsize=(8,4)):\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    ap = average_precision_score(y_true, y_score)\n",
    "    plt.plot(recall, precision, label=f'AP={ap:.3f}')\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision-Recall'); plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'AUC={roc_auc:.3f}')\n",
    "    plt.plot([0,1],[0,1],'k--', alpha=0.3)\n",
    "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC'); plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Cell 26 – confusion matrix helper\n",
    "def plot_confusion(y_true: t.Iterable, y_pred: t.Iterable, labels: t.List = None):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Pred'); plt.ylabel('True'); plt.title('Confusion Matrix')\n",
    "\n",
    "\n",
    "# Cell 27 – threshold tuning\n",
    "def threshold_tuning(y_true: t.Iterable, y_score: t.Iterable, beta: float = 2.0):\n",
    "    thresholds = np.linspace(0,1,101)\n",
    "    results = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_score = np.array(y_score)\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_score >= t).astype(int)\n",
    "        f2 = fbeta_score(y_true, y_pred, beta=beta)\n",
    "        ap = average_precision_score(y_true, y_score)\n",
    "        prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "        results.append({'threshold': t, 'fbeta': f2, 'ap': ap, 'precision': prec, 'recall': rec})\n",
    "    res_df = pd.DataFrame(results)\n",
    "    best = res_df.loc[res_df['fbeta'].idxmax()]\n",
    "    return res_df, best.to_dict()\n",
    "\n",
    "\n",
    "# Cell 28 – inference helper\n",
    "def run_inference(pipeline, X: pd.DataFrame, threshold: float = None):\n",
    "    \"\"\"Run pipeline.predict / predict_proba and return predictions and optionally labels using threshold.\n",
    "    Pipeline may be a sklearn pipeline or a trained model with predict_proba.\n",
    "    \"\"\"\n",
    "    preds = pipeline.predict(X)\n",
    "    proba = None\n",
    "    if hasattr(pipeline, 'predict_proba'):\n",
    "        try:\n",
    "            proba = pipeline.predict_proba(X)\n",
    "            # if binary, proba[:,1]\n",
    "            if proba.ndim == 2 and proba.shape[1] > 1:\n",
    "                proba = proba[:,1]\n",
    "        except Exception:\n",
    "            proba = None\n",
    "    if threshold is not None and proba is not None:\n",
    "        labels = (np.array(proba) >= threshold).astype(int)\n",
    "    elif threshold is not None:\n",
    "        labels = (np.array(preds) >= threshold).astype(int)\n",
    "    else:\n",
    "        labels = None\n",
    "    return {'pred': preds, 'proba': proba, 'label': labels}\n",
    "\n",
    "\n",
    "# Cell 29 – helper sanity tests\n",
    "def _sanity_tests():\n",
    "    print('Running simple sanity tests...')\n",
    "    import sklearn\n",
    "    # small synthetic dataset\n",
    "    df = pd.DataFrame({'YEAR':[2020,2020],'MONTH':[1,1],'DAY':[1,1],'SCHEDULED_DEPARTURE':[8,9],'DELAY_ARRIVAL':[2,5], 'SOURCE_STATION':['A','A'],'DESTINATION_STATION':['B','B'],'TRAIN_NUMBER':[1,1]})\n",
    "    rcol = _get_route_column(df)\n",
    "    df = compute_prev_delay_safe(df, groupby_cols=[rcol], dt_col=None, delay_col='DELAY_ARRIVAL')\n",
    "    print('Sanity tests passed (basic run)')\n",
    "\n",
    "# expose for import\n",
    "_helpers = {'get_route': _get_route_column, 'prev_delay': compute_prev_delay_safe, 'rolling': compute_rolling_features_safe, 'metrics_reg': metrics_summary_regression, 'metrics_clf': metrics_summary_classification}\n",
    "print('Group C helpers (Cells 21–29) added.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff06a43b",
   "metadata": {},
   "source": [
    "# Cell 30: End helper section\n",
    "\n",
    "End of Group C — helper functions. Proceed to Feature Engineering (Group D) when ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02efbbc3",
   "metadata": {},
   "source": [
    "# Group D — Feature Engineering (Cells 31–45)\n",
    "\n",
    "This group creates time features, cyclical encodings, lag and rolling features (route-based and global fallback), fills missing engineered features, performs basic checks, drops obvious leakage columns, and optionally saves an intermediate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b210b95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 31: Time features ---\n",
      "Added HOUR, DAY_OF_WEEK, MONTH to TRAIN_DF\n",
      "Added HOUR, DAY_OF_WEEK, MONTH to TEST_DF\n",
      "Cell 31 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 31 – Time features (HOUR, DAY_OF_WEEK, MONTH)\n",
    "print('\\n--- Cell 31: Time features ---')\n",
    "if TRAIN_DT_COL is not None:\n",
    "    TRAIN_DF['HOUR'] = TRAIN_DF[TRAIN_DT_COL].dt.hour\n",
    "    TRAIN_DF['DAY_OF_WEEK'] = TRAIN_DF[TRAIN_DT_COL].dt.dayofweek\n",
    "    TRAIN_DF['MONTH'] = TRAIN_DF[TRAIN_DT_COL].dt.month\n",
    "    print('Added HOUR, DAY_OF_WEEK, MONTH to TRAIN_DF')\n",
    "else:\n",
    "    print('TRAIN_DT_COL not set; cannot derive time features for train')\n",
    "\n",
    "if TEST_DF is not None and TEST_DT_COL is not None:\n",
    "    TEST_DF['HOUR'] = TEST_DF[TEST_DT_COL].dt.hour\n",
    "    TEST_DF['DAY_OF_WEEK'] = TEST_DF[TEST_DT_COL].dt.dayofweek\n",
    "    TEST_DF['MONTH'] = TEST_DF[TEST_DT_COL].dt.month\n",
    "    print('Added HOUR, DAY_OF_WEEK, MONTH to TEST_DF')\n",
    "\n",
    "print('Cell 31 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62240b01",
   "metadata": {},
   "source": [
    "# Cell 32: Cyclical encoding (SIN, COS)\n",
    "\n",
    "Encode hour and weekday as cyclical features to preserve circularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c0e9810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 32: Cyclical encoding ---\n",
      "Added HOUR_SIN, HOUR_COS\n",
      "Added DOW_SIN, DOW_COS\n",
      "Cell 32 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 32 – Cyclical encoding (SIN, COS)\n",
    "print('\\n--- Cell 32: Cyclical encoding ---')\n",
    "if 'HOUR' in TRAIN_DF.columns:\n",
    "    TRAIN_DF['HOUR_SIN'] = np.sin(2 * np.pi * TRAIN_DF['HOUR'] / 24)\n",
    "    TRAIN_DF['HOUR_COS'] = np.cos(2 * np.pi * TRAIN_DF['HOUR'] / 24)\n",
    "    print('Added HOUR_SIN, HOUR_COS')\n",
    "if 'DAY_OF_WEEK' in TRAIN_DF.columns:\n",
    "    TRAIN_DF['DOW_SIN'] = np.sin(2 * np.pi * TRAIN_DF['DAY_OF_WEEK'] / 7)\n",
    "    TRAIN_DF['DOW_COS'] = np.cos(2 * np.pi * TRAIN_DF['DAY_OF_WEEK'] / 7)\n",
    "    print('Added DOW_SIN, DOW_COS')\n",
    "\n",
    "if TEST_DF is not None:\n",
    "    if 'HOUR' in TEST_DF.columns:\n",
    "        TEST_DF['HOUR_SIN'] = np.sin(2 * np.pi * TEST_DF['HOUR'] / 24)\n",
    "        TEST_DF['HOUR_COS'] = np.cos(2 * np.pi * TEST_DF['HOUR'] / 24)\n",
    "    if 'DAY_OF_WEEK' in TEST_DF.columns:\n",
    "        TEST_DF['DOW_SIN'] = np.sin(2 * np.pi * TEST_DF['DAY_OF_WEEK'] / 7)\n",
    "        TEST_DF['DOW_COS'] = np.cos(2 * np.pi * TEST_DF['DAY_OF_WEEK'] / 7)\n",
    "\n",
    "print('Cell 32 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3916836",
   "metadata": {},
   "source": [
    "# Cell 33: Lag feature (PREV_DELAY)\n",
    "\n",
    "Compute previous delay per route/train to capture chain-effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da1a0da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 33: Compute PREV_DELAY (per route) ---\n",
      "Added PREV_DELAY per ROUTE\n",
      "Cell 33 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 33 – Lag feature (PREV_DELAY)\n",
    "print('\\n--- Cell 33: Compute PREV_DELAY (per route) ---')\n",
    "try:\n",
    "    route_col = _get_route_column(TRAIN_DF)\n",
    "    TRAIN_DF = compute_prev_delay_safe(TRAIN_DF, groupby_cols=[route_col], dt_col=TRAIN_DT_COL, delay_col=TARGET_COL, out_col='PREV_DELAY')\n",
    "    print('Added PREV_DELAY per', route_col)\n",
    "    if TEST_DF is not None:\n",
    "        try:\n",
    "            # for test, compute prev delay using train+test concatenation by route to avoid leakage issues in this quick step\n",
    "            combined = pd.concat([TRAIN_DF, TEST_DF], sort=False, ignore_index=True)\n",
    "            combined = compute_prev_delay_safe(combined, groupby_cols=[route_col], dt_col=TRAIN_DT_COL if TRAIN_DT_COL in combined.columns else TEST_DT_COL, delay_col=TARGET_COL, out_col='PREV_DELAY')\n",
    "            # copy back PREV_DELAY for test rows\n",
    "            TEST_DF['PREV_DELAY'] = combined.loc[combined.index >= len(TRAIN_DF), 'PREV_DELAY'].values\n",
    "        except Exception:\n",
    "            TEST_DF['PREV_DELAY'] = np.nan\n",
    "except Exception as e:\n",
    "    print('Could not compute PREV_DELAY:', e)\n",
    "    TRAIN_DF['PREV_DELAY'] = np.nan\n",
    "\n",
    "print('Cell 33 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a44c75",
   "metadata": {},
   "source": [
    "# Cell 34: Rolling mean 7D (route-based)\n",
    "\n",
    "Compute rolling mean/std/count for the target over a 7-day window per route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b39987f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 34: Rolling features (7D) per route ---\n",
      "Could not compute rolling features: index values must not have NaT\n",
      "Cell 34 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 34 – Rolling mean 7D (route-based)\n",
    "print('\\n--- Cell 34: Rolling features (7D) per route ---')\n",
    "try:\n",
    "    TRAIN_DF = compute_rolling_features_safe(TRAIN_DF, groupby_col=route_col, dt_col=TRAIN_DT_COL, value_col=TARGET_COL, window='7D', min_periods=1)\n",
    "    print('Added rolling features (route-based)')\n",
    "    if TEST_DF is not None:\n",
    "        # For simplicity in this quick pipeline, compute rolling on combined set and slice back\n",
    "        combined = pd.concat([TRAIN_DF, TEST_DF], sort=False, ignore_index=True)\n",
    "        combined = compute_rolling_features_safe(combined, groupby_col=route_col, dt_col=TRAIN_DT_COL if TRAIN_DT_COL in combined.columns else TEST_DT_COL, value_col=TARGET_COL, window='7D', min_periods=1)\n",
    "        TEST_DF['ROLL_MEAN_7D'] = combined.loc[combined.index >= len(TRAIN_DF), 'ROLL_MEAN_7D'].values\n",
    "        TEST_DF['ROLL_STD_7D'] = combined.loc[combined.index >= len(TRAIN_DF), 'ROLL_STD_7D'].values\n",
    "        TEST_DF['ROLL_COUNT_7D'] = combined.loc[combined.index >= len(TRAIN_DF), 'ROLL_COUNT_7D'].values\n",
    "except Exception as e:\n",
    "    print('Could not compute rolling features:', e)\n",
    "\n",
    "print('Cell 34 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0b39b",
   "metadata": {},
   "source": [
    "# Cell 35: Rolling mean global fallback\n",
    "\n",
    "Compute a global rolling mean so route-level NaNs can fall back to a global trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa627d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 35: Global rolling mean fallback (7D) ---\n",
      "Could not compute GLOBAL_ROLL_MEAN_7D: index values must not have NaT\n",
      "Cell 35 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 35 – Rolling mean global fallback\n",
    "print('\\n--- Cell 35: Global rolling mean fallback (7D) ---')\n",
    "try:\n",
    "    TRAIN_DF = TRAIN_DF.set_index(TRAIN_DT_COL)\n",
    "    TRAIN_DF['GLOBAL_ROLL_MEAN_7D'] = TRAIN_DF[TARGET_COL].rolling('7D', min_periods=1).mean()\n",
    "    TRAIN_DF = TRAIN_DF.reset_index()\n",
    "    # fill route-level ROLL_MEAN_7D with global when missing\n",
    "    if 'ROLL_MEAN_7D' in TRAIN_DF.columns:\n",
    "        TRAIN_DF['ROLL_MEAN_7D'] = TRAIN_DF['ROLL_MEAN_7D'].fillna(TRAIN_DF['GLOBAL_ROLL_MEAN_7D'])\n",
    "    print('Computed GLOBAL_ROLL_MEAN_7D and applied fallback')\n",
    "    if TEST_DF is not None:\n",
    "        try:\n",
    "            combined = pd.concat([TRAIN_DF, TEST_DF], sort=False, ignore_index=True).set_index(TRAIN_DT_COL)\n",
    "            combined['GLOBAL_ROLL_MEAN_7D'] = combined[TARGET_COL].rolling('7D', min_periods=1).mean()\n",
    "            combined = combined.reset_index()\n",
    "            TEST_DF['GLOBAL_ROLL_MEAN_7D'] = combined.loc[combined.index >= len(TRAIN_DF), 'GLOBAL_ROLL_MEAN_7D'].values\n",
    "            if 'ROLL_MEAN_7D' in TEST_DF.columns:\n",
    "                TEST_DF['ROLL_MEAN_7D'] = TEST_DF['ROLL_MEAN_7D'].fillna(TEST_DF['GLOBAL_ROLL_MEAN_7D'])\n",
    "        except Exception:\n",
    "            TEST_DF['GLOBAL_ROLL_MEAN_7D'] = np.nan\n",
    "except Exception as e:\n",
    "    print('Could not compute GLOBAL_ROLL_MEAN_7D:', e)\n",
    "\n",
    "print('Cell 35 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a967f4b",
   "metadata": {},
   "source": [
    "# Cell 36: Weather / external features (if available)\n",
    "\n",
    "Check for weather / external columns and add placeholder features or flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fb7d0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 36: External / weather features check ---\n",
      "Found weather columns: ['WEATHER_DELAY']\n",
      "Cell 36 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 36 – Weather / external features (nếu có)\n",
    "print('\\n--- Cell 36: External / weather features check ---')\n",
    "weather_cols = [c for c in TRAIN_DF.columns if 'WEATHER' in c.upper() or 'TEMPERATURE' in c.upper() or 'PRECIP' in c.upper()]\n",
    "if weather_cols:\n",
    "    print('Found weather columns:', weather_cols)\n",
    "    # example: create a simple severity flag if 'WEATHER_SEVERITY' exists\n",
    "    if 'WEATHER_SEVERITY' in TRAIN_DF.columns:\n",
    "        TRAIN_DF['WEATHER_SEV_FLAG'] = (TRAIN_DF['WEATHER_SEVERITY'] > 1).astype(int)\n",
    "        print('Added WEATHER_SEV_FLAG')\n",
    "else:\n",
    "    print('No weather/external columns detected — skipping')\n",
    "\n",
    "print('Cell 36 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e03932",
   "metadata": {},
   "source": [
    "# Cell 37: Fill missing engineered features\n",
    "\n",
    "Fill NaNs for PREV_DELAY and rolling features with sensible defaults to avoid downstream errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e81e4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 37: Filling missing engineered features ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20728\\3932380468.py:7: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  TEST_DF['PREV_DELAY'] = TEST_DF['PREV_DELAY'].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 37 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 37 – Fill missing engineered features\n",
    "print('\\n--- Cell 37: Filling missing engineered features ---')\n",
    "# PREV_DELAY: fill with 0 (assume no info => treat as 0 previous delay)\n",
    "if 'PREV_DELAY' in TRAIN_DF.columns:\n",
    "    TRAIN_DF['PREV_DELAY'] = TRAIN_DF['PREV_DELAY'].fillna(0)\n",
    "if TEST_DF is not None and 'PREV_DELAY' in TEST_DF.columns:\n",
    "    TEST_DF['PREV_DELAY'] = TEST_DF['PREV_DELAY'].fillna(0)\n",
    "\n",
    "# Rolling mean: fill with GLOBAL_ROLL_MEAN_7D or median\n",
    "for col in ['ROLL_MEAN_7D','ROLL_STD_7D','ROLL_COUNT_7D','GLOBAL_ROLL_MEAN_7D']:\n",
    "    if col in TRAIN_DF.columns:\n",
    "        TRAIN_DF[col] = TRAIN_DF[col].fillna(TRAIN_DF[col].median())\n",
    "    if TEST_DF is not None and col in TEST_DF.columns:\n",
    "        TEST_DF[col] = TEST_DF[col].fillna(TRAIN_DF[col].median() if col in TRAIN_DF.columns else 0)\n",
    "\n",
    "# Cyclical NaNs: replace with -1 which indicates missing\n",
    "for col in ['HOUR_SIN','HOUR_COS','DOW_SIN','DOW_COS']:\n",
    "    if col in TRAIN_DF.columns:\n",
    "        TRAIN_DF[col] = TRAIN_DF[col].fillna(-1)\n",
    "    if TEST_DF is not None and col in TEST_DF.columns:\n",
    "        TEST_DF[col] = TEST_DF[col].fillna(-1)\n",
    "\n",
    "print('Cell 37 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1105d18",
   "metadata": {},
   "source": [
    "# Cell 38: Feature distribution check\n",
    "\n",
    "Inspect distributions of the engineered features (describe + optional histograms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50bd0147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 38: Feature distribution check ---\n",
      "Engineered columns count: 7\n",
      "                              count      mean       std  min  25%  50%  \\\n",
      "SCHEDULED_DEPARTURE_HOUR  2592697.0  0.000000  0.000000  0.0  0.0  0.0   \n",
      "HOUR                      2592697.0  0.000000  0.000000  0.0  0.0  0.0   \n",
      "HOUR_SIN                  9312671.0 -0.721595  0.448214 -1.0 -1.0 -1.0   \n",
      "HOUR_COS                  9312671.0 -0.443189  0.896428 -1.0 -1.0 -1.0   \n",
      "DOW_SIN                   9312671.0 -0.722138  0.583850 -1.0 -1.0 -1.0   \n",
      "DOW_COS                   9312671.0 -0.730736  0.570311 -1.0 -1.0 -1.0   \n",
      "\n",
      "                               75%       max  \n",
      "SCHEDULED_DEPARTURE_HOUR  0.000000  0.000000  \n",
      "HOUR                      0.000000  0.000000  \n",
      "HOUR_SIN                  0.000000  0.000000  \n",
      "HOUR_COS                  1.000000  1.000000  \n",
      "DOW_SIN                  -0.974928  0.974928  \n",
      "DOW_COS                  -0.900969  1.000000  \n",
      "Cell 38 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 38 – Feature distribution check\n",
    "print('\\n--- Cell 38: Feature distribution check ---')\n",
    "eng_cols = [c for c in TRAIN_DF.columns if any(pat in c for pat in ['PREV_DELAY','ROLL_','HOUR','DOW','GLOBAL_ROLL'])]\n",
    "print('Engineered columns count:', len(eng_cols))\n",
    "print(TRAIN_DF[eng_cols].describe().T)\n",
    "\n",
    "# Optional quick plots (commented out by default)\n",
    "# for c in eng_cols:\n",
    "#     plt.figure(); sns.histplot(TRAIN_DF[c].dropna(), bins=50); plt.title(c)\n",
    "\n",
    "print('Cell 38 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40383713",
   "metadata": {},
   "source": [
    "# Cell 39: Drop leakage columns\n",
    "\n",
    "Remove columns that clearly leak future information (e.g., ACTUAL_* columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7836469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 39: Drop leakage columns ---\n",
      "Leakage candidate columns to drop: ['ACTUAL_DEPARTURE', 'LEFT_SOURCE_STATION_TIME', 'ACTUAL_ARRIVAL']\n",
      "Dropped leakage columns\n",
      "Cell 39 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 39 – Drop leakage columns\n",
    "print('\\n--- Cell 39: Drop leakage columns ---')\n",
    "leak_cols = [c for c in TRAIN_DF.columns if c.upper().startswith('ACTUAL_') or c.upper().startswith('LEFT_')]\n",
    "# do not drop target or scheduled columns\n",
    "leak_cols = [c for c in leak_cols if c not in [TRAIN_DT_COL, TARGET_COL]]\n",
    "print('Leakage candidate columns to drop:', leak_cols)\n",
    "if leak_cols:\n",
    "    TRAIN_DF = TRAIN_DF.drop(columns=leak_cols)\n",
    "    if TEST_DF is not None:\n",
    "        TEST_DF = TEST_DF.drop(columns=[c for c in leak_cols if c in TEST_DF.columns], errors='ignore')\n",
    "    print('Dropped leakage columns')\n",
    "else:\n",
    "    print('No obvious leakage columns found')\n",
    "\n",
    "print('Cell 39 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5911c168",
   "metadata": {},
   "source": [
    "# Cell 40: Feature list snapshot\n",
    "\n",
    "Take a snapshot of feature column names for downstream modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f04c837b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 40: Feature list snapshot ---\n",
      "Total columns: 36 Feature candidate count: 35\n",
      "First 50 features: ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'TRAIN_OPERATOR', 'TRAIN_NUMBER', 'COACH_ID', 'SOURCE_STATION', 'DESTINATION_STATION', 'DELAY_DEPARTURE', 'PLATFORM_TIME_OUT', 'TRAIN_DEPARTURE_EVENT', 'SCHEDULED_TIME', 'ELAPSED_TIME', 'RUN_TIME', 'DISTANCE_KM', 'PLATFORM_TIME_IN', 'SCHEDULED_ARRIVAL', 'DIVERTED', 'CANCELLED', 'CANCELLATION_REASON', 'SYSTEM_DELAY', 'SECURITY_DELAY', 'TRAIN_OPERATOR_DELAY', 'LATE_TRAIN_DELAY', 'WEATHER_DELAY', 'SCHEDULED_DEPARTURE_HOUR', 'SCHEDULED_DEPARTURE_WEEKDAY', 'HOUR', 'HOUR_SIN', 'HOUR_COS', 'DOW_SIN', 'DOW_COS', 'ROUTE', 'PREV_DELAY']\n",
      "Cell 40 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 40 – Feature list snapshot\n",
    "print('\\n--- Cell 40: Feature list snapshot ---')\n",
    "reserved = {TARGET_COL, TRAIN_DT_COL}\n",
    "features = [c for c in TRAIN_DF.columns if c not in reserved]\n",
    "print('Total columns:', len(TRAIN_DF.columns), 'Feature candidate count:', len(features))\n",
    "print('First 50 features:', features[:50])\n",
    "FEATURE_LIST = features  # expose for downstream\n",
    "print('Cell 40 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f863c2",
   "metadata": {},
   "source": [
    "# Cell 41: Feature sanity check\n",
    "\n",
    "Verify missingness and types for the main engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d1f4b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 41: Feature sanity check ---\n",
      "Present engineered columns: ['PREV_DELAY', 'HOUR', 'DAY_OF_WEEK']\n",
      "PREV_DELAY: dtype=object, NA%=0.00\n",
      "HOUR: dtype=float64, NA%=72.16\n",
      "DAY_OF_WEEK: dtype=float64, NA%=72.16\n",
      "Cell 41 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 41 – Feature sanity check\n",
    "print('\\n--- Cell 41: Feature sanity check ---')\n",
    "check_cols = ['PREV_DELAY','ROLL_MEAN_7D','ROLL_STD_7D','ROLL_COUNT_7D','GLOBAL_ROLL_MEAN_7D','HOUR','DAY_OF_WEEK']\n",
    "present = [c for c in check_cols if c in TRAIN_DF.columns]\n",
    "print('Present engineered columns:', present)\n",
    "for c in present:\n",
    "    pct_na = TRAIN_DF[c].isna().mean() * 100\n",
    "    print(f\"{c}: dtype={TRAIN_DF[c].dtype}, NA%={pct_na:.2f}\")\n",
    "print('Cell 41 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e23050",
   "metadata": {},
   "source": [
    "# Cell 42: Print engineered columns\n",
    "\n",
    "List columns created by feature engineering for quick inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "428671af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 42: Engineered columns ---\n",
      "['DAY_OF_WEEK', 'DOW_COS', 'DOW_SIN', 'HOUR', 'HOUR_COS', 'HOUR_SIN', 'PREV_DELAY', 'SCHEDULED_DEPARTURE_HOUR']\n",
      "Cell 42 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 42 – Print engineered columns\n",
    "eng_cols = [c for c in TRAIN_DF.columns if any(pat in c for pat in ['PREV_DELAY','ROLL_','GLOBAL_ROLL','HOUR_','DOW_','HOUR','DAY_OF_WEEK'])]\n",
    "print('\\n--- Cell 42: Engineered columns ---')\n",
    "print(sorted(set(eng_cols)))\n",
    "print('Cell 42 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ef350",
   "metadata": {},
   "source": [
    "# Cell 43: Memory cleanup\n",
    "\n",
    "Drop temporary variables and run garbage collection to free memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd57ff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 43: Memory cleanup ---\n",
      "Memory cleanup done\n",
      "Cell 43 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 43 – Memory cleanup\n",
    "print('\\n--- Cell 43: Memory cleanup ---')\n",
    "import gc\n",
    "# drop large temporaries if present\n",
    "for name in ['combined','tmp']:\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "gc.collect()\n",
    "print('Memory cleanup done')\n",
    "print('Cell 43 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b4219",
   "metadata": {},
   "source": [
    "# Cell 44: Save intermediate df (optional)\n",
    "\n",
    "Save processed train dataframe to a parquet file for faster iteration/use in downstream steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "739a2f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 44: Save intermediate dataframe (optional) ---\n",
      "Could not save TRAIN_DF: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.\n",
      "Cell 44 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 44 – Save intermediate df (optional)\n",
    "print('\\n--- Cell 44: Save intermediate dataframe (optional) ---')\n",
    "OUT_DIR = os.path.join('data','interim')\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "out_path = os.path.join(OUT_DIR, 'train_fe.parquet')\n",
    "try:\n",
    "    TRAIN_DF.to_parquet(out_path, index=False)\n",
    "    print('Saved intermediate TRAIN_DF to', out_path)\n",
    "except Exception as e:\n",
    "    print('Could not save TRAIN_DF:', e)\n",
    "\n",
    "print('Cell 44 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400ad96",
   "metadata": {},
   "source": [
    "# Cell 45: End feature engineering marker\n",
    "\n",
    "End of Group D — Feature Engineering. Proceed to Group E (Target & Feature Split) when ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75768a97",
   "metadata": {},
   "source": [
    "# Group E — Target & Feature Split (Cells 46–50)\n",
    "\n",
    "Define features/target, optionally log-transform the target, save an original copy, and print basic target statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e750576b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 46: Define X and y ---\n",
      "Using 35 features (sample): ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'TRAIN_OPERATOR', 'TRAIN_NUMBER', 'COACH_ID', 'SOURCE_STATION', 'DESTINATION_STATION', 'DELAY_DEPARTURE', 'PLATFORM_TIME_OUT', 'TRAIN_DEPARTURE_EVENT', 'SCHEDULED_TIME', 'ELAPSED_TIME', 'RUN_TIME', 'DISTANCE_KM', 'PLATFORM_TIME_IN', 'SCHEDULED_ARRIVAL', 'DIVERTED', 'CANCELLED']\n",
      "Defined X_train (shape= (9312671, 35) ) and y_train (length= 9312671 )\n",
      "Cell 46 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 46 – Define X, y\n",
    "print('\\n--- Cell 46: Define X and y ---')\n",
    "# Ensure FEATURE_LIST exists; if not, compute conservatively\n",
    "if 'FEATURE_LIST' not in globals() or FEATURE_LIST is None:\n",
    "    reserved = {TARGET_COL, TRAIN_DT_COL}\n",
    "    FEATURE_LIST = [c for c in TRAIN_DF.columns if c not in reserved]\n",
    "\n",
    "print('Using', len(FEATURE_LIST), 'features (sample):', FEATURE_LIST[:20])\n",
    "X = TRAIN_DF[FEATURE_LIST].copy()\n",
    "y = TRAIN_DF[TARGET_COL].copy()\n",
    "X_train = X\n",
    "y_train = y\n",
    "print('Defined X_train (shape=', X_train.shape, ') and y_train (length=', len(y_train), ')')\n",
    "print('Cell 46 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc750d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 47: Optional log-transform of target ---\n",
      "LOG_TRANSFORM_TARGET is False — no transform applied\n",
      "Cell 47 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 47 – Log-transform target (nếu có)\n",
    "print('\\n--- Cell 47: Optional log-transform of target ---')\n",
    "# Toggle this to True to apply log1p transform to the target for modeling stability\n",
    "LOG_TRANSFORM_TARGET = False\n",
    "if LOG_TRANSFORM_TARGET:\n",
    "    TRAIN_DF['TARGET_LOG'] = np.log1p(TRAIN_DF[TARGET_COL].clip(lower=0))\n",
    "    y_train = TRAIN_DF['TARGET_LOG']\n",
    "    print('Applied log1p transform to target; sample stats:')\n",
    "    print(TRAIN_DF['TARGET_LOG'].describe())\n",
    "else:\n",
    "    print('LOG_TRANSFORM_TARGET is False — no transform applied')\n",
    "print('Cell 47 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29faecd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 48: Save original target column ---\n",
      "Saved original target to TRAIN_DF[\"TARGET_ORIG\"]\n",
      "Cell 48 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 48 – Save original target\n",
    "print('\\n--- Cell 48: Save original target column ---')\n",
    "if 'TARGET_ORIG' not in TRAIN_DF.columns:\n",
    "    TRAIN_DF['TARGET_ORIG'] = TRAIN_DF[TARGET_COL].copy()\n",
    "    print('Saved original target to TRAIN_DF[\"TARGET_ORIG\"]')\n",
    "else:\n",
    "    print('TARGET_ORIG already exists; skipping')\n",
    "\n",
    "print('Cell 48 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d77c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 49 – Target stats\n",
    "print('\\n--- Cell 49: Target statistics ---')\n",
    "if TARGET_COL in TRAIN_DF.columns:\n",
    "    raw_t = TRAIN_DF[TARGET_COL]\n",
    "    # detect non-numeric entries before coercion\n",
    "    non_numeric_mask = pd.to_numeric(raw_t, errors='coerce').isna() & raw_t.notna()\n",
    "    n_non_numeric = int(non_numeric_mask.sum())\n",
    "    if n_non_numeric > 0:\n",
    "        print('Non-numeric target entries (count):', n_non_numeric)\n",
    "        print('Sample non-numeric values:')\n",
    "        print(raw_t[non_numeric_mask].unique()[:10])\n",
    "    # coerce to numeric for statistics (invalid -> NaN -> dropped)\n",
    "    t = pd.to_numeric(raw_t, errors='coerce').dropna()\n",
    "    print('Count (numeric):', len(t))\n",
    "    if len(t) > 0:\n",
    "        print(t.describe())\n",
    "        print('Skew:', t.skew())\n",
    "        print('Negative values (count):', (t < 0).sum())\n",
    "        print('Zeros (count):', (t == 0).sum())\n",
    "        print('Quantiles (1%, 5%, 25%, 50%, 75%, 95%, 99%):', t.quantile([0.01,0.05,0.25,0.5,0.75,0.95,0.99]).to_dict())\n",
    "    else:\n",
    "        print('No numeric target values after coercion.')\n",
    "else:\n",
    "    print('TARGET_COL not found in TRAIN_DF')\n",
    "\n",
    "print('Cell 49 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835d42a8",
   "metadata": {},
   "source": [
    "# Cell 50: End target section\n",
    "\n",
    "End of Group E — Target & Feature Split. Proceed to Group F (Feature types & Preprocessing) when ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732b19b3",
   "metadata": {},
   "source": [
    "# Group F — Feature Types & Preprocessing (Cells 51–60)\n",
    "\n",
    "Detect numeric/categorical/label features, check for leakage, and build numeric and categorical pipelines plus a ColumnTransformer preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e279c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 51: Detect numeric features ---\n",
      "Detected numeric feature count: 9\n",
      "Sample numeric features: ['MONTH', 'DAY_OF_WEEK', 'SCHEDULED_DEPARTURE_HOUR', 'SCHEDULED_DEPARTURE_WEEKDAY', 'HOUR', 'HOUR_SIN', 'HOUR_COS', 'DOW_SIN', 'DOW_COS']\n",
      "Cell 51 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 51 – Detect numeric features\n",
    "print('\\n--- Cell 51: Detect numeric features ---')\n",
    "NUMERIC_FEATURES = [c for c in FEATURE_LIST if pd.api.types.is_numeric_dtype(TRAIN_DF[c])]\n",
    "print('Detected numeric feature count:', len(NUMERIC_FEATURES))\n",
    "print('Sample numeric features:', NUMERIC_FEATURES[:20])\n",
    "print('Cell 51 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2dd2dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 52: Detect categorical features ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20728\\416887090.py:4: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  CAT_DTYPE = [c for c in FEATURE_LIST if pd.api.types.is_string_dtype(TRAIN_DF[c]) or pd.api.types.is_categorical_dtype(TRAIN_DF[c])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected categorical feature count: 18\n",
      "Sample categorical features: ['CANCELLED', 'DAY', 'DAY_OF_WEEK', 'DISTANCE_KM', 'DIVERTED', 'DOW_COS', 'DOW_SIN', 'HOUR', 'HOUR_COS', 'HOUR_SIN', 'MONTH', 'ROUTE', 'SCHEDULED_ARRIVAL', 'SCHEDULED_DEPARTURE_HOUR', 'SCHEDULED_DEPARTURE_WEEKDAY', 'SOURCE_STATION', 'TRAIN_NUMBER', 'YEAR']\n",
      "Cell 52 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 52 – Detect categorical features\n",
    "print('\\n--- Cell 52: Detect categorical features ---')\n",
    "# object and category dtypes\n",
    "CAT_DTYPE = [c for c in FEATURE_LIST if pd.api.types.is_string_dtype(TRAIN_DF[c]) or pd.api.types.is_categorical_dtype(TRAIN_DF[c])]\n",
    "# low-cardinality numeric may be categorical\n",
    "LOW_CARD_THRESH = 50\n",
    "CAT_FROM_NUM = [c for c in FEATURE_LIST if pd.api.types.is_numeric_dtype(TRAIN_DF[c]) and TRAIN_DF[c].nunique(dropna=True) <= LOW_CARD_THRESH]\n",
    "CATEGORICAL_FEATURES = sorted(set(CAT_DTYPE + CAT_FROM_NUM))\n",
    "print('Detected categorical feature count:', len(CATEGORICAL_FEATURES))\n",
    "print('Sample categorical features:', CATEGORICAL_FEATURES[:20])\n",
    "print('Cell 52 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e60b6ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 53: Detect label-encoded features ---\n",
      "Label-encoded candidates (<=10 unique ints): []\n",
      "Cell 53 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 53 – Detect label-encoded features\n",
    "print('\\n--- Cell 53: Detect label-encoded features ---')\n",
    "LABEL_ENCODE_FEATURES = [c for c in FEATURE_LIST if pd.api.types.is_integer_dtype(TRAIN_DF[c]) and TRAIN_DF[c].nunique(dropna=True) <= 10]\n",
    "print('Label-encoded candidates (<=10 unique ints):', LABEL_ENCODE_FEATURES)\n",
    "print('Cell 53 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "abc5dbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 54: Leakage check ---\n",
      "Potential leakage columns (contain \"DELAY\" but are not target): ['DELAY_DEPARTURE', 'SYSTEM_DELAY', 'SECURITY_DELAY', 'TRAIN_OPERATOR_DELAY', 'LATE_TRAIN_DELAY', 'WEATHER_DELAY', 'PREV_DELAY']\n",
      "Cell 54 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 54 – Leakage check (DELAY_MINUTES)\n",
    "print('\\n--- Cell 54: Leakage check ---')\n",
    "leakage_candidates = [c for c in TRAIN_DF.columns if 'DELAY' in c.upper() and c != TARGET_COL]\n",
    "if leakage_candidates:\n",
    "    print('Potential leakage columns (contain \"DELAY\" but are not target):', leakage_candidates)\n",
    "else:\n",
    "    print('No obvious delay leakage columns found')\n",
    "print('Cell 54 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22797a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 55: Numeric transformer ---\n",
      "Numeric transformer ready; steps: ['imputer']\n",
      "Cell 55 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 55 – Numeric transformer\n",
    "print('\\n--- Cell 55: Numeric transformer ---')\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SCALE_NUMERIC = False  # set True if you want StandardScaler for numeric features\n",
    "numeric_steps = [('imputer', SimpleImputer(strategy='median'))]\n",
    "if SCALE_NUMERIC:\n",
    "    numeric_steps.append(('scaler', StandardScaler()))\n",
    "NUMERIC_TRANSFORMER = Pipeline(steps=numeric_steps)\n",
    "print('Numeric transformer ready; steps:', [s[0] for s in numeric_steps])\n",
    "print('Cell 55 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ca1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 56: Categorical transformer ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m cat_ohe = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m CATEGORICAL_FEATURES \u001b[38;5;28;01mif\u001b[39;00m TRAIN_DF[c].nunique(dropna=\u001b[38;5;28;01mTrue\u001b[39;00m) <= CARDINALITY_THRESHOLD]\n\u001b[32m      8\u001b[39m cat_ord = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m CATEGORICAL_FEATURES \u001b[38;5;28;01mif\u001b[39;00m TRAIN_DF[c].nunique(dropna=\u001b[38;5;28;01mTrue\u001b[39;00m) > CARDINALITY_THRESHOLD]\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m cat_ohe_steps = [(\u001b[33m'\u001b[39m\u001b[33mimputer\u001b[39m\u001b[33m'\u001b[39m, SimpleImputer(strategy=\u001b[33m'\u001b[39m\u001b[33mmost_frequent\u001b[39m\u001b[33m'\u001b[39m)), (\u001b[33m'\u001b[39m\u001b[33mohe\u001b[39m\u001b[33m'\u001b[39m, \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mignore\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)]\n\u001b[32m     11\u001b[39m cat_ord_steps = [(\u001b[33m'\u001b[39m\u001b[33mimputer\u001b[39m\u001b[33m'\u001b[39m, SimpleImputer(strategy=\u001b[33m'\u001b[39m\u001b[33mmost_frequent\u001b[39m\u001b[33m'\u001b[39m)), (\u001b[33m'\u001b[39m\u001b[33mord\u001b[39m\u001b[33m'\u001b[39m, OrdinalEncoder(handle_unknown=\u001b[33m'\u001b[39m\u001b[33muse_encoded_value\u001b[39m\u001b[33m'\u001b[39m, unknown_value=-\u001b[32m1\u001b[39m))]\n\u001b[32m     13\u001b[39m CATEGORICAL_TRANSFORMER = {\u001b[33m'\u001b[39m\u001b[33mohe_cols\u001b[39m\u001b[33m'\u001b[39m: cat_ohe, \u001b[33m'\u001b[39m\u001b[33mord_cols\u001b[39m\u001b[33m'\u001b[39m: cat_ord, \u001b[33m'\u001b[39m\u001b[33mohe_pipeline\u001b[39m\u001b[33m'\u001b[39m: Pipeline(cat_ohe_steps), \u001b[33m'\u001b[39m\u001b[33mord_pipeline\u001b[39m\u001b[33m'\u001b[39m: Pipeline(cat_ord_steps)}\n",
      "\u001b[31mTypeError\u001b[39m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "# Cell 56 – Categorical transformer\n",
    "print('\\n--- Cell 56: Categorical transformer ---')\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# For high-cardinality categorical, use OrdinalEncoder to avoid explosion — else OneHot\n",
    "CARDINALITY_THRESHOLD = 50\n",
    "cat_ohe = [c for c in CATEGORICAL_FEATURES if TRAIN_DF[c].nunique(dropna=True) <= CARDINALITY_THRESHOLD]\n",
    "cat_ord = [c for c in CATEGORICAL_FEATURES if TRAIN_DF[c].nunique(dropna=True) > CARDINALITY_THRESHOLD]\n",
    "\n",
    "# instantiate OneHotEncoder compatibly across sklearn versions\n",
    "try:\n",
    "    ohe_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "except TypeError:\n",
    "    ohe_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "cat_ohe_steps = [('imputer', SimpleImputer(strategy='most_frequent')), ('ohe', ohe_encoder)]\n",
    "cat_ord_steps = [('imputer', SimpleImputer(strategy='most_frequent')), ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))]\n",
    "\n",
    "CATEGORICAL_TRANSFORMER = {'ohe_cols': cat_ohe, 'ord_cols': cat_ord, 'ohe_pipeline': Pipeline(cat_ohe_steps), 'ord_pipeline': Pipeline(cat_ord_steps)}\n",
    "print('Categorical OHE columns:', len(cat_ohe), 'Ordinal columns:', len(cat_ord))\n",
    "print('Cell 56 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 57 – ColumnTransformer (preprocessor)\n",
    "print('\\n--- Cell 57: Build ColumnTransformer preprocessor ---')\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "transformers = []\n",
    "if len(NUMERIC_FEATURES) > 0:\n",
    "    transformers.append(('num', NUMERIC_TRANSFORMER, NUMERIC_FEATURES))\n",
    "if len(CATEGORICAL_TRANSFORMER['ohe_cols']) > 0:\n",
    "    transformers.append(('cat_ohe', CATEGORICAL_TRANSFORMER['ohe_pipeline'], CATEGORICAL_TRANSFORMER['ohe_cols']))\n",
    "if len(CATEGORICAL_TRANSFORMER['ord_cols']) > 0:\n",
    "    transformers.append(('cat_ord', CATEGORICAL_TRANSFORMER['ord_pipeline'], CATEGORICAL_TRANSFORMER['ord_cols']))\n",
    "\n",
    "if not transformers:\n",
    "    raise ValueError('No transformers defined — check FEATURE_LIST and detected feature types')\n",
    "\n",
    "PREPROCESSOR = ColumnTransformer(transformers=transformers, remainder='drop', verbose_feature_names_out=False)\n",
    "print('Preprocessor created with transformers:', [t[0] for t in transformers])\n",
    "print('Cell 57 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 58 – Preprocessor sanity check\n",
    "print('\\n--- Cell 58: Preprocessor sanity check ---')\n",
    "sample = X_train.sample(n=min(2000, len(X_train)), random_state=RANDOM_STATE)\n",
    "try:\n",
    "    PREPROCESSOR.fit(sample)\n",
    "    out = PREPROCESSOR.transform(sample)\n",
    "    print('Preprocessor fit/transform successful; transformed shape sample ->', out.shape)\n",
    "except Exception as e:\n",
    "    print('Preprocessor fit/transform FAILED:', e)\n",
    "\n",
    "print('Cell 58 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3fde88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 59 – Feature count check\n",
    "print('\\n--- Cell 59: Feature count check ---')\n",
    "try:\n",
    "    transformed = PREPROCESSOR.transform(sample)\n",
    "    if hasattr(transformed, 'shape'):\n",
    "        print('Transformed feature count:', transformed.shape[1])\n",
    "    else:\n",
    "        print('Transformed output type:', type(transformed))\n",
    "except Exception as e:\n",
    "    print('Could not compute transformed feature count:', e)\n",
    "\n",
    "print('Cell 59 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29bedfc",
   "metadata": {},
   "source": [
    "# Cell 60: End preprocessing section\n",
    "\n",
    "End of Group F — preprocessing. Proceed to Group G (Split & CV) when ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a985cc5",
   "metadata": {},
   "source": [
    "# Group G — Split & CV (Cells 61–65)\n",
    "\n",
    "Time-aware train/validation split, define TimeSeriesSplit, and perform a CV sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a5a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 61 – Train / validation split (shuffle=False)\n",
    "print('\\n--- Cell 61: Train / validation split (time-aware) ---')\n",
    "VAL_FRACTION = 0.2  # fraction for validation set\n",
    "if TRAIN_DT_COL in TRAIN_DF.columns and TRAIN_DF[TRAIN_DT_COL].notna().sum() > 0:\n",
    "    cutoff = TRAIN_DF[TRAIN_DT_COL].quantile(1 - VAL_FRACTION)\n",
    "    train_mask = TRAIN_DF[TRAIN_DT_COL] <= cutoff\n",
    "    val_mask = TRAIN_DF[TRAIN_DT_COL] > cutoff\n",
    "    X_tr = X_train.loc[train_mask].reset_index(drop=True)\n",
    "    y_tr = y_train.loc[train_mask].reset_index(drop=True)\n",
    "    X_val = X_train.loc[val_mask].reset_index(drop=True)\n",
    "    y_val = y_train.loc[val_mask].reset_index(drop=True)\n",
    "    print(f\"Time split by cutoff {cutoff} -> train: {len(X_tr)}, val: {len(X_val)}\")\n",
    "else:\n",
    "    # fallback to simple index split\n",
    "    split_idx = int(len(X_train) * (1 - VAL_FRACTION))\n",
    "    X_tr = X_train.iloc[:split_idx].reset_index(drop=True)\n",
    "    y_tr = y_train.iloc[:split_idx].reset_index(drop=True)\n",
    "    X_val = X_train.iloc[split_idx:].reset_index(drop=True)\n",
    "    y_val = y_train.iloc[split_idx:].reset_index(drop=True)\n",
    "    print(f\"Index split -> train: {len(X_tr)}, val: {len(X_val)}\")\n",
    "\n",
    "print('Cell 61 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd17f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 62 – Print split sizes\n",
    "print('\\n--- Cell 62: Print split sizes ---')\n",
    "print('X_tr shape:', X_tr.shape)\n",
    "print('y_tr length:', len(y_tr))\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('y_val length:', len(y_val))\n",
    "print('Cell 62 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb5142",
   "metadata": {},
   "source": [
    "# Cell 63: TimeSeriesSplit definition (tscv)\n",
    "\n",
    "Define a TimeSeriesSplit for time-aware CV and print split counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 63 – TimeSeriesSplit definition (tscv)\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "N_SPLITS = 5\n",
    "tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "print(f'Created TimeSeriesSplit with {N_SPLITS} splits')\n",
    "print('Sample fold sizes (on training portion):')\n",
    "for i, (tr_idx, val_idx) in enumerate(tscv.split(X_tr)):\n",
    "    print(f'Fold {i+1}: train {len(tr_idx)}, val {len(val_idx)}')\n",
    "    if i >= 4:\n",
    "        break\n",
    "print('Cell 63 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c9ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 64 – CV sanity check\n",
    "print('\\n--- Cell 64: CV sanity check (quick pipeline run on first fold) ---')\n",
    "from sklearn.linear_model import LinearRegression\n",
    "try:\n",
    "    # use first fold\n",
    "    tr_idx, val_idx = next(iter(tscv.split(X_tr)))\n",
    "    X_tr_fold = X_tr.iloc[tr_idx]\n",
    "    y_tr_fold = y_tr.iloc[tr_idx]\n",
    "    X_val_fold = X_tr.iloc[val_idx]\n",
    "    y_val_fold = y_tr.iloc[val_idx]\n",
    "\n",
    "    lr_sm = pipeline.Pipeline([('preprocessor', PREPROCESSOR), ('lr', LinearRegression())])\n",
    "    lr_sm.fit(X_tr_fold, y_tr_fold)\n",
    "    preds = lr_sm.predict(X_val_fold)\n",
    "    m = metrics_summary_regression(y_val_fold, preds)\n",
    "    print('Fold 1 quick LR metrics:', m)\n",
    "except Exception as e:\n",
    "    print('CV sanity check failed:', e)\n",
    "\n",
    "print('Cell 64 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a9a6b5",
   "metadata": {},
   "source": [
    "# Cell 65: End split section\n",
    "\n",
    "End of Group G — split & CV. Proceed to Group H (Baseline models) when ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b318a0f",
   "metadata": {},
   "source": [
    "# Group H — Baseline Models (Cells 66–70)\n",
    "\n",
    "Simple baselines: Linear regression and Random Forest, evaluate on validation set, save baseline results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0abbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 66 – Linear regression baseline\n",
    "print('\\n--- Cell 66: Linear regression baseline ---')\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_pipe = pipeline.Pipeline([('preprocessor', PREPROCESSOR), ('lr', LinearRegression())])\n",
    "lr_pipe.fit(X_tr, y_tr)\n",
    "lr_preds = lr_pipe.predict(X_val)\n",
    "lr_metrics = metrics_summary_regression(y_val, lr_preds)\n",
    "print('Linear Regression metrics on validation:', lr_metrics)\n",
    "\n",
    "print('Cell 66 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef69fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 67 – Random Forest baseline\n",
    "print('\\n--- Cell 67: Random Forest baseline ---')\n",
    "rf_pipe = pipeline.Pipeline([('preprocessor', PREPROCESSOR), ('rf', RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1))])\n",
    "rf_pipe.fit(X_tr, y_tr)\n",
    "rf_preds = rf_pipe.predict(X_val)\n",
    "rf_metrics = metrics_summary_regression(y_val, rf_preds)\n",
    "print('Random Forest metrics on validation:', rf_metrics)\n",
    "\n",
    "print('Cell 67 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c309e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 68 – Baseline evaluation\n",
    "print('\\n--- Cell 68: Baseline evaluation ---')\n",
    "baseline_results = []\n",
    "baseline_results.append({'model': 'median_baseline', **metrics_summary_regression(y_val, np.repeat(y_tr.median(), len(y_val)))})\n",
    "baseline_results.append({'model': 'linear_regression', **lr_metrics})\n",
    "baseline_results.append({'model': 'random_forest', **rf_metrics})\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_results).set_index('model')\n",
    "print('\\nBaseline comparison:')\n",
    "print(baseline_df)\n",
    "\n",
    "print('Cell 68 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 69 – Save baseline results\n",
    "print('\\n--- Cell 69: Save baseline results ---')\n",
    "OUT_REPORTS = 'reports'\n",
    "os.makedirs(OUT_REPORTS, exist_ok=True)\n",
    "baseline_path = os.path.join(OUT_REPORTS, 'baseline_results.csv')\n",
    "try:\n",
    "    baseline_df.to_csv(baseline_path)\n",
    "    print('Saved baseline results to', baseline_path)\n",
    "except Exception as e:\n",
    "    print('Could not save baseline results:', e)\n",
    "\n",
    "print('Cell 69 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3928a9",
   "metadata": {},
   "source": [
    "# Cell 70: End baseline section\n",
    "\n",
    "End of Group H — Baseline Models. Proceed to Group I (Tuning & Advanced Models) when ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c08a95",
   "metadata": {},
   "source": [
    "# Group I — Tuning & Advanced Models (Cells 71–78)\n",
    "\n",
    "Set up Optuna objective, parameter space, create study, run optimization, summarize best params, train tuned model, validate and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c4bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 71 – Define Optuna objective\n",
    "print('\\n--- Cell 71: Define Optuna objective (LGBM) ---')\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "N_TRIALS = 20  # default; adjust as needed\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 16, 128),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 15),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 200),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0)\n",
    "    }\n",
    "\n",
    "    # time-series CV using tscv\n",
    "    rmses = []\n",
    "    for tr_idx, val_idx in tscv.split(X_tr):\n",
    "        X_tr_fold = X_tr.iloc[tr_idx]\n",
    "        y_tr_fold = y_tr.iloc[tr_idx]\n",
    "        X_val_fold = X_tr.iloc[val_idx]\n",
    "        y_val_fold = y_tr.iloc[val_idx]\n",
    "\n",
    "        pipe = pipeline.Pipeline([\n",
    "            ('preprocessor', PREPROCESSOR),\n",
    "            ('model', LGBMRegressor(random_state=RANDOM_STATE, **params))\n",
    "        ])\n",
    "        pipe.fit(X_tr_fold, y_tr_fold)\n",
    "        preds = pipe.predict(X_val_fold)\n",
    "        rmse = mean_squared_error(y_val_fold, preds, squared=False)\n",
    "        rmses.append(rmse)\n",
    "    return float(np.mean(rmses))\n",
    "\n",
    "print('Objective defined. Adjust N_TRIALS and run Cell 74 to optimize.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d132e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 72 – Define parameter space (documented)\n",
    "print('\\n--- Cell 72: Parameter space (documented) ---')\n",
    "print('Using LGBM parameter search: n_estimators, learning_rate, num_leaves, max_depth, min_child_samples, subsample, colsample_bytree, reg_alpha, reg_lambda')\n",
    "print('You can customize ranges in Cell 71 (objective).')\n",
    "print('Cell 72 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736aa3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 73 – Create Optuna study\n",
    "print('\\n--- Cell 73: Create Optuna study ---')\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=RANDOM_STATE))\n",
    "print('Study created. Run Cell 74 to start optimization (this may take time depending on N_TRIALS).')\n",
    "print('Cell 73 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a4d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 74 – Run Optuna optimization\n",
    "print('\\n--- Cell 74: Run Optuna optimization ---')\n",
    "# WARNING: may take time. Adjust N_TRIALS to a small value for quick runs.\n",
    "N_TRIALS = 20\n",
    "try:\n",
    "    study.optimize(objective, n_trials=N_TRIALS)\n",
    "    print('Optimization completed')\n",
    "except Exception as e:\n",
    "    print('Optuna optimization failed or was interrupted:', e)\n",
    "\n",
    "print('Cell 74 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2368c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 75 – Best params summary\n",
    "print('\\n--- Cell 75: Best params summary ---')\n",
    "try:\n",
    "    print('Best value (RMSE):', study.best_value)\n",
    "    print('Best params:')\n",
    "    print(study.best_params)\n",
    "    BEST_PARAMS = study.best_params\n",
    "except Exception as e:\n",
    "    print('Could not read study results:', e)\n",
    "    BEST_PARAMS = None\n",
    "\n",
    "print('Cell 75 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28bc9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 76 – Train tuned model\n",
    "print('\\n--- Cell 76: Train tuned model with best params ---')\n",
    "if BEST_PARAMS is not None:\n",
    "    try:\n",
    "        tuned_pipe = pipeline.Pipeline([\n",
    "            ('preprocessor', PREPROCESSOR),\n",
    "            ('model', LGBMRegressor(random_state=RANDOM_STATE, **BEST_PARAMS))\n",
    "        ])\n",
    "        tuned_pipe.fit(X_tr, y_tr)\n",
    "        print('Tuned model trained on training portion.')\n",
    "    except Exception as e:\n",
    "        print('Training tuned model failed:', e)\n",
    "        tuned_pipe = None\n",
    "else:\n",
    "    print('BEST_PARAMS is None — cannot train tuned model')\n",
    "    tuned_pipe = None\n",
    "\n",
    "print('Cell 76 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 77 – Validation prediction\n",
    "print('\\n--- Cell 77: Validation prediction with tuned model ---')\n",
    "if tuned_pipe is not None:\n",
    "    val_preds_tuned = tuned_pipe.predict(X_val)\n",
    "    print('Predicted on validation set; sample predictions:')\n",
    "    print(val_preds_tuned[:5])\n",
    "else:\n",
    "    print('Tuned model not available; skipping predictions')\n",
    "\n",
    "print('Cell 77 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3757196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 78 – Metrics calculation for tuned model\n",
    "print('\\n--- Cell 78: Metrics calculation for tuned model ---')\n",
    "if tuned_pipe is not None:\n",
    "    tuned_metrics = metrics_summary_regression(y_val, val_preds_tuned)\n",
    "    print('Tuned model metrics on validation:', tuned_metrics)\n",
    "    # append to baseline_df for comparison\n",
    "    try:\n",
    "        baseline_df.loc['tuned_lgbm'] = tuned_metrics\n",
    "    except Exception:\n",
    "        baseline_df = baseline_df.append(pd.Series(tuned_metrics, name='tuned_lgbm'))\n",
    "else:\n",
    "    print('Tuned model not available; skipping metrics calculation')\n",
    "\n",
    "print('Cell 78 completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b16d3ab",
   "metadata": {},
   "source": [
    "# Group J — Finalization (Cells 79–82)\n",
    "\n",
    "Compare models, inspect feature importance or SHAP, save the best model, and run inference on the test set (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49311990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 79 – Model comparison table\n",
    "print('\\n--- Cell 79: Model comparison table ---')\n",
    "try:\n",
    "    print(baseline_df.sort_values('rmse'))\n",
    "except Exception as e:\n",
    "    print('Could not display baseline_df:', e)\n",
    "\n",
    "print('Cell 79 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b429a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 80 – Feature importance / SHAP\n",
    "print('\\n--- Cell 80: Feature importance / SHAP ---')\n",
    "if tuned_pipe is not None:\n",
    "    try:\n",
    "        model = tuned_pipe.named_steps['model']\n",
    "        # try to get feature names\n",
    "        try:\n",
    "            feat_names = PREPROCESSOR.get_feature_names_out(FEATURE_LIST)\n",
    "        except Exception:\n",
    "            try:\n",
    "                feat_names = PREPROCESSOR.get_feature_names_out()\n",
    "            except Exception:\n",
    "                feat_names = [f'f{i}' for i in range(model.feature_importances_.shape[0])]\n",
    "\n",
    "        importances = model.feature_importances_\n",
    "        fi = pd.DataFrame({'feature': feat_names, 'importance': importances})\n",
    "        fi = fi.sort_values('importance', ascending=False).head(30)\n",
    "        print('Top feature importances:')\n",
    "        print(fi)\n",
    "    except Exception as e:\n",
    "        print('Could not compute feature importances:', e)\n",
    "    # SHAP (optional)\n",
    "    try:\n",
    "        import shap\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        # sample for SHAP due to cost\n",
    "        sample_X = X_val.sample(n=min(500, len(X_val)), random_state=RANDOM_STATE)\n",
    "        X_trans = PREPROCESSOR.transform(sample_X)\n",
    "        shap_values = explainer.shap_values(X_trans)\n",
    "        print('SHAP computed for sample; consider plotting (not shown here).')\n",
    "    except Exception as e:\n",
    "        print('SHAP not available or failed:', e)\n",
    "else:\n",
    "    print('tuned_pipe not available; skipping feature importance')\n",
    "\n",
    "print('Cell 80 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc483f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 81 – Save best model (joblib.dump)\n",
    "print('\\n--- Cell 81: Save best model ---')\n",
    "if tuned_pipe is not None:\n",
    "    out_path = os.path.join(MODEL_DIR, 'best_model.joblib')\n",
    "    try:\n",
    "        joblib.dump(tuned_pipe, out_path)\n",
    "        print('Saved best model to', out_path)\n",
    "    except Exception as e:\n",
    "        print('Could not save model:', e)\n",
    "else:\n",
    "    print('No tuned model to save')\n",
    "\n",
    "print('Cell 81 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 82 – Inference on test / end notebook\n",
    "print('\\n--- Cell 82: Inference on test (if available) ---')\n",
    "if TEST_DF is not None and 'PREV_DELAY' in TEST_DF.columns:\n",
    "    X_test = TEST_DF[FEATURE_LIST].copy()\n",
    "    if tuned_pipe is not None:\n",
    "        try:\n",
    "            pred_test = tuned_pipe.predict(X_test)\n",
    "            out_df = TEST_DF.copy()\n",
    "            out_df['PRED_DELAY'] = pred_test\n",
    "            out_path = os.path.join('reports', 'test_predictions.csv')\n",
    "            out_df.to_csv(out_path, index=False)\n",
    "            print('Saved test predictions to', out_path)\n",
    "        except Exception as e:\n",
    "            print('Could not run inference on test:', e)\n",
    "    else:\n",
    "        print('No tuned model; skipping test inference')\n",
    "else:\n",
    "    print('No test set available or required features missing; skipping inference')\n",
    "\n",
    "print('Cell 82 completed — END of notebook pipeline.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5c0e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
